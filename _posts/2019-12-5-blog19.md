---

title: "MemGuard:Defending against Black-Box Membership Inference Attacks via Adversial Examples"
author_profile: true
date: 2019-11-28
tags: [AI security]
mathjax: "true"
header:
    image: "/imgs/blog19.png"
excerpt: "AI security"
---

## 文章探讨的主题：利用特殊噪声的样本进行模型防御

## 文章的创新点：对构造出的优化问题的转换

## MemGuard的主要背景和思想

现在有一个模型，比如说卷积神经网络分类任务模型。

现在有一张图片，是一张猫猫的图片。

现在有一个Attacker，想要窃取原始数据的部分信息，这里就是想知道猫猫图片的部分特征或者部分信息，比如说毛的颜色，眼睛的颜色等

现在Attacker，可以自己构造一个模型，比如决策树，神经网络或者是其他模型，使用这个模型对得到的母模型数据进行学习，如果模型良好，那么黑客模型能够训练学习得到原始数据的部分特征。



**这样就不好，我的原始数据，也就是我的猫猫特征被Attacker泄漏。**



我母模型的防护方法并没有在我的模型上动手脚，而是在我的样本上动手脚。

在我的样本上精心构造出一个噪声，将噪声施加到我的样本上，形成了一个新的样本，看上去仍然是猫猫，但实际上添加了噪声信号在图像中。

这个添加完噪声之后的样本要满足几个条件：

* 对母模型的干扰尽可能小，底线是不能成为母模型的对抗样本

* 对黑客模型的干扰尽可能大，“大”的衡量标准是如果新的样本在黑客模型中使得黑客模型产生的接近随机结果那就是最大的干扰。

  **从本质上来说，就是想要把黑客拿到的信息度降低，模型的信息度增高，也就是说在保证母模型信息熵增加小于阈值的情况下使得黑客模型的信息熵尽可能大**



那么我想要保护模型，我就要模仿自己是黑客，进行“渗透测试”。

自己建立一个攻击模型，这个模型可以是各种类型的模型，但是都起到了攻击母体模型的作用。通过自己建立的黑客模型来测试自己的母模型在噪声样本的稳定性情况。【自己攻击自己】



##MemGuard的优化问题数学转换方法

这就转换成了一个优化问题：

寻找噪声干扰的半径r

**优化问题：**

$$\underset{r} min \quad d(s,s+r)$$        //使干扰噪声最小，s代表的是置信分数向量

$$s.t. \underset{J}{\operatorname{argmax}}(s_j+r_j) = \underset{J}{\operatorname{argmax}}{s_j}$$     //代表生成的含噪声样本并没有作为母模型的对抗样本

​		$$g(s+r) = 0.5$$     //g代表的是黑客模型，黑客模型输出的是随机结果，也就是最大熵的情况

​		$$s_j +r_j >= 0,\forall j$$   

​		$$\sum_j{r_j} = 0$$   //这两个式子保证了加入噪声r之后仍然是一个概率分布



对优化问题进行转换，通过限制 噪声 r 的概率分布来进行subject to 条件的简化。

两个代表性的噪声向量是0 和 r，考虑随机噪声分布是介于0-r之间的一个噪声向量，我们对这个噪声分布进行**特殊化定义**，目的是将优化问题进行简化。

定义这个噪声概率分布是 p 的概率取到 0 , (1-p) 的概率取到 r。

这样优化问题就转换成了对超参 p 的寻找。对于 p ，我们想要使得在 r 的概率分布下，黑客模型的结果尽可能接近0.5。

$$\underset{p}{\operatorname{argmin}}|p\cdot g(s+r) + (1-p)\cdot g(s+0) - 0.5|$$

$$s.t.  p\cdot d(s,s+r) + (1-p)\cdot d(s,s+0) <= \epsilon$$

//在保证噪声概率分布产生的新样本不会成为母模型对抗样本的情况下尽可能地让黑客模型的结果接近随机，也就是在母模型信息熵增量小于对抗样本增量的情况下尽可能地增加黑客模型的样本信息熵



## MemGuard 的效果

黑客模型是神经网络模型（NN）、随机森林模型（RF）等模型时噪声样本对黑客模型的信息熵均有不小的增加作用。

在母模型容许的对抗阈值增加的情况下，黑客模型的推断准确度会逐渐降低。





