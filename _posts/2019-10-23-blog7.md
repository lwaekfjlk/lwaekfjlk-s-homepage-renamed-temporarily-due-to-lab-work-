---
title: "Adversial Attacks on Deep Learning Models in Natural Language Processing"
author_profile: true
date: 2019-10-23
tags: [Natural Language Processing，Adversial Attack]
mathjax: "true"
header:
    image: "/imgs/blog7"
excerpt: "Natural Language Processing，Adversial Attack"
---

## 本文探讨的主题：NLP和CV中对模型的对抗攻击

## 本文想要解决的问题：指出对抗攻击的含义并指出NLP与CV中对抗攻击的不同之处

## 对抗攻击

指使用对抗样例干扰噪声对DNN模型发起攻击。对抗样例中包含了难以察觉的扰动噪声，欺骗DNN，使得通过在样例上施加扰动噪声让DNN得到错误的预测。（对分类器来说就是给出错误的分类结果）

由于连续实值的向量表示（例如嵌入），DNN非常擅长处理包含多种形式的数据（图像，文本，视频，音频）。

但是，DNN的黑箱模型导致**无法直观地感受到每个神经元到底学了什么内容**。这种黑箱模型带来了一系列问题，比如DNN的稳定性不佳。通过使用微小扰动噪声的方法测量DNN的稳定性，发现DNN受扰动因素干扰很大，而相比之下人类判断没有影响。

## NLP和CV在DNN稳定性测量方面的本质不同

* **连续 vs 离散**

  CV 图像数据是连续的（像素连续），NLP文本数据是离散的。文本数据的离散性决定了在向量空间中文本数据的稀疏性。

  NLP的对抗样例主要是不合法的单词，例如plcae和hallo。离散性和稀疏性使得对抗样例在做词嵌入的时候不会和词向量空间的其他词有所匹配。

* **易察觉 vs 不易察觉**

  CV中微小的干扰往往不易被察觉，但是NLP中微小的对抗样本很容易就会被察觉发现。

* **富语义性 vs 乏语义性**

  NLP中的对抗样本会极大地在语义层面改变句子本身。

此三点导致了两者在做对抗处理的时候方法完全不同。

## 对抗攻击对稳定性的定量分析

1. 对抗攻击的数学定义建立在干扰和输入和输出都进行向量化的基础上。

   数学上是对$x$施加一个干扰$\eta$ , s.t. $x' = x + \eta$，这样一个扰动 $\eta$ 效果是使得 $f(x)$ 从原本的 $ y $ 变为后来的 $y'$ 。其中 $x$ 是样本向量化的产物，扰动也是向量化的结果表示，而 $f$ 则代表了被攻击的DNN模型。 

2. 被攻击的模型根据模型是否已知模型细节分为**黑盒攻击**（结构、参数未知）和**白盒攻击**（结构、参数已知）。

3. 对于攻击目的，分为**无目的攻击和有目的攻击**。

   无目标攻击（untargeted attack）：对于一张图片，生成一个对抗样本，使得标注系统在其上的标注与原标注无关，即只要攻击成功就好，对抗样本的最终属于哪一类不做限制。

   有目标攻击（targeted attack）：对于一张图片和一个目标标注句子，生成一个对抗样本，使得标注系统在其上的标注与目标标注完全一致，即不仅要求攻击成功，还要求生成的对抗样本属于特定的类。

4. 从**攻击细粒度**角度看

   攻击模型的被攻发生层面对于CV来说细粒度是像素级（扰动发生在像素上）而对NLP来说是字符集或者单词集或者句子级嵌入攻击（扰动和微小改变发生在不同的细粒度级别，效果也各不相同）。

5. 被攻击后需要衡量两个值，一个是干扰噪声的大小，一个是被攻击后的影响效果。通过对这两个指标的衡量能够具体地定量给出了一个模型的稳定性。

   * **对于干扰噪声的扰动大小衡量**，干扰噪声定义为不改变模型结果和输入标签却对DNN造成最大影响的 $\eta$ 。

     （如果以分类器为例子，就是说对模型施加干扰之后虽然最终的分类结果没有改变，但是各个类别的百分比出现了变化，原结果的分类百分比被削弱，尽管结果没有变化，例如正确的分类准确度从98%降到了60%，最终结果没变）

     理想化地来说，最好的噪声扰动是限制范围内的最大值。

     从直观上来说，噪声扰动既不能过大也不能过小，过大会影响模型结果或改变模型标签，过小DNN又没有非常明显的影响，没有达到攻击和干扰模型的效果。

     **在CV中**，通过对 $\eta$ 做一些限制，例如**控制 $\eta$ 的L0，L2，或者无穷范数**来设置干扰噪声的限制，在限制内进行求值。由于图像数据的稠密规整，可以得到较好结果。

     **在NLP中**，在对文本**向量化**之后，若通过范数的方式来限制扰动常常非法且无法理解，因为NLP中扰动涉及到了语法，语义，相似性等多方面因素。

     所以NLP中限制和衡量干扰常常使用多个标准综合衡量：

     **1）语法相关性衡量**（语法检查器）

     **2）语义是否保持不变衡量**（欧氏空间词向量距离）

     **3）使用编辑距离衡量字符串相似性**（编辑距离衡量字符串细粒度）

   * **对于被攻击后的影响效果的量化**，一般的方法可以通过准确率来进行衡量，但是NLP影响效果是一个开放性问题，与之前提及的一样，干扰样本对NLP模型的影响可能是多方面的。

   ## 对NLP模型的具体攻击

   ![blog7](/imgs/blog7_content.png)