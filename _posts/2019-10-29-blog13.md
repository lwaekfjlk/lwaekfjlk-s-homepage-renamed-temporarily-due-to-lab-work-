---

title: "GloVe：Global Vectors for Word Representation"
author_profile: true
date: 2019-10-29
tags: [Natural Language Processing, word embedding]
mathjax: "true"
header:
    image: "/imgs/blog13.jpg"
excerpt: "Natural Language Processing, word embeddimg"
---

## 本文探讨的主题：GloVe模型

## 本文想要解决的问题：结合矩阵分解和局部窗口移动的方法来形成更好的模型

## GloVe

GloVe 模型是在Skip-Gram和CBOW模型提出一年后提出的词向量训练模型

区别于Skip-Gram和CBOW【predicted 模型】，GloVe【通过共现矩阵进行的count-based回归模型】的训练并没有使用神经网络，而是计算共现矩阵通过统计的方法来得到的。

**GloVe模型，从本质上来说是一种学习词向量的log-bilinear 回归模型，用来使用非监督的方法学习词向量，并且在词语推断，词语相似性和命名实体识别三个NLP任务中都超越了其他模型。**

[其中，log-双线性模型的叫法是因为模型目标不是什么 $P(w0|w1,w1...)  = \frac{exp} {\sum exp}$，模型中对e的次方取了log 操作，并且log之后的结果是两个矩阵的乘积操作，也就是相当于矩阵相乘。一个**双线性**映射是由两个向量空间上的元素，生成第三个向量空间上一个元素之函数，并且该函数对每个参数都是线性的。]

##GloVe基本思想

之前测量word embedding质量的方法都是根据词向量空间叫和距离来判断相不相似。Mikolov 提出了利用线性性作为加减法词向量来做 **类比**的测试方式。

学习词向量的方法大致有两类：

* 全局矩阵分解

  以LSA为代表，模型缺点是在词语类比任务中表现非常差，说明学得的词向量空间的线性结构较弱

  *矩阵分解方法利用了低秩近似【low-rank approximation】来分解代表语料库统计信息的超大矩阵，超大矩阵有两种，term-document矩阵的行是单词，列是语料库中的不同文件，term-term矩阵的行是单词列也是单词，矩阵点上的值代表了i作为中心词，j出现在周围的次数*

* 局部文本窗口方法

  以word2vec为代笔，在类比任务上效果更好，但是没有使用文本语料库的统计数据，而且word2vec训练出来的词向量的线性性解释不清楚

  *利用神经网络的方法把词向量的训练从下游具体任务中分离了出来，这类模型被实验证明了生成的词向量是具有线性关系的，模型根据窗口扫描语料库，但是没有使用数据中大量的重复数据，也就是**全局矩阵分解中的出现次数信息并没有被使用起来**，只是考虑了局部的窗口信息【上下文特诊】*

GloVe的优越之处在于这个模型融合了全局矩阵分解和局部文本窗口方法。

###精华思想：

吸收全局矩阵分解的地方在于 **构建了 共享概率模型之后 构建词向量和共现矩阵之间的近似关系**。和LSA相比，glove可看作是对LSA一种优化的高效矩阵分解算法，采用Adagrad对最小平方损失进行优化；

吸收局部文本窗口的地方在于 **共享概率模型的构建是按照一定范围进行构建的，矩阵中的每一个元素都是在上下文的一定范围内进行构建的**。

## GloVe模型的具体推导

- 共现概率矩阵的构造

  X 的元素 $X_{i,j}$是语料若是出现在word i 上下文中的word j 的次数

  $X_i = \sum_{k}X_{i,k}$是出现在word i 上下文中的所有word 的总次数

  $P_{i,j} = P(j|i) = X_{i,j}/X_{i}$是word j 出现在哦word i 上下文的概率

  共现概率矩阵：

  行定义为语料库中的各个单词k

  列定义为 P(k|i) 代表k出现在i周围的概率

- 共现概率矩阵中的ratio i,j,k

  Ratio 定义为 $\frac{P_{i,k}}{P_{j,k}}$

  Ratio i,j,k的值

  | Ratio i,j,k 的值 | 单词j,k相关 | 单词j,k不相关 |
  | ---------------- | ----------- | ------------- |
  | 单词i,k相关      | 接近1       | 很大          |

    单词i,k不相关                                            很小 					                            趋近1

  相比于原始的概率，ratio值更能区分出相关的单词组和不相关的单词组并且更好的区分两个相关单词的区别。

  所以说词向量学习的出发点应该是 **共现概率的比值**而不是 **共现概率**本身。

- 模型构建的开始

  既然要使用Ratio，而且 $\frac{P_{i,k}}{P_{j,k}}$ 和 $w_i,w_j,\widetilde{w_k}$ 有关 

  【$w$ 和 $\widetilde{w}$ 是两个分离的矩阵$W$ 和 $\widetilde{W}$ ，他们的唯一区别就是训练时的初始值不同,为什么要这样做呢？

  这是一个训练中的小trick，有证据证明用多个样例进行训练去平均值可以增加准确率，所以我们用这两个矩阵的道理也是一样的，把$W+\widetilde{W}$作为真正的词向量能够进一步提高准确度，**增加了不同的随机噪声，增加了鲁棒性**】

  那么，模型就可以模糊地写成

  $F(w_i,w_j,\widetilde{w_k}) = \frac{P_{i,k}}{P_{j,k}}$

  **这个模型的右侧是统计得出的，左侧是我们想要求出的词向量，同时函数是未知的，我们认为F是通过编码词向量信息来表示统计信息的**。

  这里的思想和CBOW一样，知道了周围词的词向量和中心词的向量，对周围词的词向量放进一层神经网络得到的预测结果和word i 在分类器中所处的位置相同。

- 函数的优化【也就是对F函数进行限制，让F满足一定条件】

  第一步，

  既然词向量所在的向量空间是天然具有线性结构的。我们想要构造出的词向量所在的向量空间要有线性结构。

  要在有线性关系的向量空间中衡量向量之间的 similarity，那么最简单的方法就是 **相减**。

  所以我们只要考虑两个词向量相减作为一个向量就可以在线性向量空间中判定词向量的相似程度。

  $F((w_i-w_j),\widetilde{w_k}) = \frac{P_{i,k}}{P_{j,k}}$

  

  第二步，

  右边是标量，左边是矢量，这样的一个 F 映射构建出来会混淆 线性关系。为了保证词向量之间的线性关系，做内积操作，把两个向量变成一个标量。

  $F((w_i-w_j)^{T}\cdot\widetilde{w_k}) = \frac{P_{i,k}}{P_{j,k}}$

  这样就防止了F变换把向量变成我们不想要的东西，防止 F 混合乱混合向量维度。

  

  第三步，

  由于右边是统计量，在统计矩阵中，中心词和上下文是可以互换的，一个词可以在一种情况做上下文词一种情况做中心词，所以 F 所作用的参数必须要使得 $w$ 和$\widetilde{w}$，$X$和$X^T$可以互换。但是第二步中得到的公式不行。

  所以得进一步转换。

  这种对称性可以通过两步转换：

  1. 要求 F 是 实加法群和 正实数乘法群的 同态【e的指数运算】

     加法群 (R, +) 和 正实数乘法群（R+，*）

     f(a + b) = f(a) * f(b)

     这样的同态函数很显然是指数运算。

     解出$w_i^{T}\widetilde{w_k} = log(P_{i,k}) = log(X_{i,k})-log(X_i)$  

  2. 从上个步骤的解中可以看出如果没有$log(X_i)$，就有了上述所要求的交换性，但是$log(X_i)$与 k 无关，所以变成$b_i$

     最后再添加一项 $\widetilde{b_k}$【保持模型的对称性】就可以完成对称性的转换。

     这么多步骤终于把$F(w_i,w_j,\widetilde{w_k}) = \frac{P_{i,k}}{P_{j,k}}$转换成了$w_i^{T}\widetilde{w_k}+b_i\widetilde{b_k} = log(X_{i,k}) $

  

- 开始回归

  把$log(X_{i,k})$变成$log(X_{i,k}+1)$能够解决上述优化公式在 0处没有定义的问题，优化$w_i^{T}\widetilde{w_k}+b_i\widetilde{b_k} = log(X_{i,k}+1) $的思路和LSA差不多，但是这样有些单词出现地多，包含的信息少这点没有体现。

  所以我们修改一下

  $J = \sum_{i,j=1}^{V}f(X_{i,j})(w_i^{T}\widetilde{w_k}+b_i\widetilde{b_k} -log(X_{i,k})) $

  V是整个词典的单词个数 $X_{i,k}$ 为 0 时，规定 f(X) = 0,这样就实现了**又有权重，又解决了不良定义**。

  $X_{i,k}$ 是已知的，其他可以回归。

##和其他模型的关系

从思路上说：

所有用非监督学习方法学习词向量的方法都统一地基于**语料库中出现次数的统计**。

所以不同模型从本质上应该是一样的。

文中从word2vec模型的cost function直接推出了glove模型的cost function，非常精彩。

skip-gram中

$J = - \sum_{i\in corpsus,j\in context(i)}logQ_{i,j}$

$Q_{i,j} = \frac{e^{w_i^T\widetilde{w_k}}}{\sum_{k=1}^{V}e^{w_i^T\widetilde{w_k}}}$

Skip-gram使用了softmax的近似版分层softmax作为简化cost function的计算。

变成了$J = - \sum_{i=1}^V\sum_{j=1}^VX_{i,j}logQ_{i,j}$

把$X_{i,j}$利用定义展开，变成了$J = - \sum_{i=1}^VX_i\sum_{j=1}^VP_{i,j}logQ_{i,j} = \sum_{i=1}^VX_i\sum_{j=1}^VH(P_i,Q_i)$

$H(P_i,Q_i)$交叉熵对应了两个分布之间的距离，那么不用交叉熵用 最小二乘也能定义类似的两个分布之间的距离。

所以不妨把交叉熵换掉：

$\hat{J} = \sum_{i=1}^VX_i\sum_{j=1}^V(\hat{P_{i,j}}-\hat{Q_{i,j}})^2$

其中 $\hat{P_{i,j}}$代表的概率分布也就是真实的分布，$X_{i,j}$

其中 $\hat{Q_{i,j}}$代表的概率分布也就是我们训练出的分布，$e^{w_i^{T}\widetilde{w_k}}$

这两个值太大，会影响优化效果，所以都去log

$\hat{J} = \sum_{i=1}^VX_i\sum_{j=1}^V(log\hat{P_{i,j}}-log\hat{Q_{i,j}})^2 = \sum_{i=1}^VX_i\sum_{j=1}^V({w_i^{T}\widetilde{w_k}}-log{w_i^{T}\widetilde{w_k}})^2$

这样就转换成了glove类似的cost function。



## 模型复杂度

首先得出$<O(|V|^2)$，之后压缩证明真正的更小上界为O(|C|)。

和Skip-Gram相比，在较好的情况下GloVe模型的复杂度甚至要更加大一点，而且GloVe的并行性更好。

