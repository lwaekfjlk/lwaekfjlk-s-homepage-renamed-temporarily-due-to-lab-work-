---

title: "word2vec Explained：Deriving Mikolov et al‘s Negative-Sampling Word-Embedding Method"
author_profile: true
date: 2019-10-25
tags: [Natural Language Processing, word2vec]
mathjax: "true"
header:
    image: "/imgs/blog9.jpeg"
excerpt: "Natural Language Processing, word2vec"
---

## 本文探讨的主题：Negative-Sampling

## 本文想要解决的问题：从Skip-Gram模型出发理解Negative-Sampling

## Skip-Gram 模型的优化目标

在这个模型中，我们知道模型任务是根据中心词，预测周围的上下文词。把中心词看作语料库中的一个w，把上下文看作文章c，那么我们的模型任务就是给定一个Text，通过调整参数 $\theta$ 的方法来最大化模型预测正确的概率。

$$argmax_{\theta} \prod_{w \in Text}[\prod_{c \in C(w)} p(c|w;\theta)]$$



$p(c|w;\theta)$代表了预测一个上下文位置 C 正确的概率。

$\prod_{c \in C(w)} p(c|w;\theta)$代表了预测整篇文章正确的概率。

$\prod_{w \in Text}[\prod_{c \in C(w)} p(c|w;\theta)]$代表了根据每一单词都能预测整篇文章的概率。

这个式子过于复杂，我们规定一个集合$D = {(w,c)}$这个集合是所有中心词和上下文的集合。

**注意这个D都是从文章中抽取出来的组合，换句话说，这个D是预测模型的正确答案**

那么优化的目标就简化为了

$$argmax_{\theta}\prod_{(w,c)\in D}p(c,w;\theta)$$



## Skip-Gram模型优化目标的进一步分析

由于 $p(c|w;\theta)$是通过softmax 得来的，所以

$$p(c|w;\theta) = \frac{e^{v_c \cdot v_w}}{\sum_{c' \in C}e^{v_c \cdot v_w}}$$

于是将优化目标取对数转换

$argmax_{\theta} \sum_{(w,c)\in D}log p(c|w) = \sum_{(w,c) \in D}log e^{v_c \cdot v_w}-log\sum_{c'}e^{v_c' \cdot v_w}$



*（非常关键的一个假定）*

接下来假定：

**如果这个优化目标最大，那么就意味着词向量input vector从相似单词有相似向量的意义上看，word embedding 的效果好。**

这个假定成立的原因并不是很清楚。

## Hierarchical Softmax

$\sum_{c'}e^{v_c' \cdot v_w}$这个东西在softmax中的存在导致当C很大的时候，softmax 非常地大，所以可以使用分层softmax来进行计算。

## Negative Sampling

nagative sampling虽然是建立在skip-gram模型上的，但是他和skip-gram模型并不是优化的同一个东西。

首先考虑（w，c）这个对，这个单词对是否在context中？

这个可能性使用 $p(D=1|w,c)$衡量，D=0时的概率就代表了(w,c)单词对不在context中的概率。

假设参数 $\theta$ 也是控制 $p(D=1|w,c;\theta)$,那么我们的目标就是最大化我们模型结果在context内的概率。也就是说最大化预测正确的概率。 

$$argmax_{\theta}\prod p(D=1|w,c;\theta)$$

和先前一样进行log的转换，并且将$p(D=1|w,c;\theta)$定义为 $\frac{1}{1+e^{v_c \cdot v_w}}$

优化目标成功转换为

$$argmax_{\theta}\sum_{(w,c) \in D}log \frac{1}{1+e^{v_c \cdot v_m}}$$

但是这个优化目标是有问题的。只要$v_c \cdot v_w$够大并且$v_c = v_w$我们就可以轻易地达成这个优化目标。那么从直观的角度来说，$v_c \cdot v_w$足够大就意味着word 和 context 是非常匹配的，有许多上下文重叠的words相似度是非常高的，有很多单词重叠的文章相似度十分叉高的。这样的优化使得语料库中所有的单词和上下文组成的单词对之间都非常地匹配，各部分趋同。

所以面对这个优化问题，我们需要通过不允许一些(w,c)组合来防止所有的向量都是一个值。一个方法就是抽取一些 $p(D=1|w,c;\theta)$很小的(w,c)单词对来对优化目标过拟合的情况进行**正则**。

通过随机选择(w,c) 单词对生成 D' 集合来加入模型，假设他们是不对的，D'集合中的就是负样本。

这样的话，得到正则化后的优化目标就变化成了：

$$argmax_{\theta}\prod_{(w,c) \in D} p(D=1|w,c;\theta)\prod_{(w,c) \in D'}  p(D=0|w,c;\theta)$$

从直观上来说，优化目标变成了最大化在文章中的单词对预测正确的概率和不在文章中的单词队预测不正确的概率。也就是说防止了模型训练出来发现所有的单词都是在context中的单词，也就是说使得模型通过词向量明显地区分出了什么是context中得出的单词对，什么是不在context中得出的单词对，而不是统一地让不在文中的单词对和在文中的单词对变得统一。

这就是引入负采样的真正原因。

## Negative Sampling 具体优化 

和Skip-gram的优化不一样，skip-gram对 p(c|w) 建模，而negative sampling 通过 w 和 c 的联合分布来对一个数 D’集合的个数 进行建模。

如果有 要有 K 个负样本，我们就 构造一个比D大K倍的负样本集合D’，对于每个D中的 (w,c)，我们构造K个单词对作为负样本 (w,c1),(w,c2),(w,c3)...(w,ck)。

每一个$c_j$是根据单一分布来进行决定的。每个单词对(w,c)成为 D'集合一员的概率是 遵循 $p_{words}(w) \frac{p_{contexts}(c)^{\frac{3}{4}}}{Z}$的概率分布。$p_{words}$ 和 $p_{contexts}$ 是 words 和 contexts的 出现频次分布。Z是一个正则系数。

如果我们固定了 中心词的向量表示，只学习周围词的向量表示或者固定了周围词的向量表示，只学习中心词的向量表示，那么这个word2vec模型就会退化成logistic regression并且是凸的问题。但是同时学习 input vector 和 output vector 或者说同时学习中心词向量表示和周围词的向量表示就会使这个问题成为非凸问题，word2vec收敛于局部最优的原因正在于此。