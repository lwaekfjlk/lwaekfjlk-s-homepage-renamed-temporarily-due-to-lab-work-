---
title: "A Comprehensive Survey on Graph Neural Networks"
author_profile: true
date: 2019-09-27
tags: [Graph Neural Networks]
mathjax: "true"
header:
    image: "/imgs/blog2.jpeg"
excerpt: "Graph Neural Networks"
---

## **本篇论文探讨的主题**：GNN综述

## **本篇论文的主要想法**：

* 图网络能够处理更为广泛的非欧的数据。（为什么图结构是一种非欧结构？）

  （什么是readout？）

* GNN 本质上是一种将图数据（非欧结构）作用于神经网络的产物。

* 典型的图数据：

   1)电子商务中**用户与产品**的关系；

   2)化学中**分子**作为图;

   3)引用网络中**被引论文**的关系；

  图节点的个性化，邻居关系的不同导致了无法在图结构上使用传统CNN。 

* GNN的分类（根据输出）：

  1）结点层面（输出与结点回归与分类有关，RecGNN 和 ConvGNN就是通过信息传播/图卷积提取抽象性结点表示的结构，本质上是结点层面的GNN）

  2）边层面（输出与边分类和链接预测有关，可以预测两点间边的标签和边的权值）

  3）图层面（输出与图分类有关，pooling和readout使用）

* GNN的分类（根据训练模式）：

  1）结点层面的半监督训练模式：结点部分标注，部分未标注，ConvGNN可以预测无标签结点的标签

  2）图层面的监督训练：监督训练用于预测整个图的标签（ConvGNN+Pooling+Readout）

  3）图嵌入的非监督学习：图上完全没有标签可以使用，边层面的信息被利用。利用方式有两种，**其一**是用自编码器将图直接转换成隐性表达，**其二**是采用**负样本+logistic回归层**

* GNN的分类（根据模型思想）：

  1）RecGNN;

  2）ConvGNN;（包括谱方法和空间方法两类）

  3）GAE;（包括图嵌入和图生成两类）

  4）Spatial-temporal Graph(结点输入根据时间变化的带属性图)



## 一、RecGNN

​	RecGNN 是GNN 最早的研究方向。

​	具体地说，RecGNN的基本思想是**通过以迭代的方式传播结点邻居的信息来学习结点的特征知道学得的解收敛于一个定值**。

​	其中，一个图中的结点与其邻居**交换/传播信息**直到**迭代结果收敛到定植**，迭代的过程中**循环**使用一组参数。（信息传播的思想来源于RNN的结构特征，被spatial-based ConvGNN所继承思想）

RecGNN的迭代更新公式如下：

$$h_v^{(t)} = \sum_{u\in N(v)}f(X_v,X_{(u,v)}^{e},X_u,h_u^{(t-1)})$$

如何理解这个基本公式？

1. v结点的**隐藏**特征向量，由他一个映射迭代决定，真正的隐藏特征向量是迭代的收敛点。而迭代只与自己，邻居，自己与邻居的关系有关。
2. 循环映射函数的参数包含了**自身的外在特征信息，邻居的外在特征信息，邻边的外在特征信息，t-1时刻的邻居隐藏特征信息**。
3. 这个映射是压缩映像，保证了**迭代收敛性**。若这个映射是神经网络，压缩性表现在正则化上。
4. 这个映射参数是**恒定不变**的，所以这个映射才叫做**循环函数**。这是RecGNN的一个最大特点，也是他与ConvGNN的最主要区别。

RecGNN的模型异同：

1. GNN*,没看懂

2. GraphESN,没看懂

3. GGNN,使用了GRU作为循环函数，不懂

4. SSE,不懂

   

## 二、ConvGNN

图卷积神经网络来源于CNN的成功。

图卷积神经网络两类：第一类谱是源于基于谱图理论的图卷积定义

​									第二类源于RecCNN的信息传播方法理论

图卷积神经网络储存了多个**不同**的图卷积层来提取图节点的高抽象特征。图卷积是很多复杂网络的重要组成部分，在结点层面和图层面的GNN都有所应用。

对于图中的**循环相互依赖性**RecCNN使用了**压缩正则**来进行迭代，**循环性特征**包含在了固定的映射规则中，变化的只是映射参数。

相较之，ConcGNN使用了**固定层数的多层卷积层**来处理**图迭代过程中出现的循环相互依赖性**



### **第一类：spectral-based GNN**

​	理解spectral-based 的步骤：

​    **STEP 1**:拉普拉斯矩阵 L = D - A (D为度矩阵，对角代表结点的度，A为邻接矩阵[01矩阵])

​	**STEP 2**:拉普拉斯矩阵的正则化(我也不知道为什么要这样啊？？？)

​					$$D^{-\frac{1}{2}}LD^{-\frac{1}{2}} = D^{-\frac{1}{2}}DD^{-\frac{1}{2}}-D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$$

​					$$L' = D^{-\frac{1}{2}}LD^{-\frac{1}{2}}= I_n -D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$$ 

​	**STEP 3**:对角化（我并不知道是怎么搞的。。。）

​					$$L = U\Lambda U^T$$

​				 $$U$$成为了正交矩阵，列向量是傅立叶变换的基，$$\Lambda$$是对角矩阵，对角元素代表着标准正则拉普拉斯矩阵的特征值    			  （ 特征值就是传说中的**谱(spectral)**）

​				在图像信号处理中，$x \in R^n$是图中所有结点的特征向量，$x_i$ 代表了第i个结点的特征值。

​	**STEP 4**:傅立叶变换（**非欧空间<-->频谱正交基构成的空间**）

​				注意，这个转换是可逆的。

​				一个$x$从非欧空间出发，变到频谱空间，在频谱空间上定义的卷积对频谱进行滤波，

​				然后再返回非欧空间，就得到了经过卷积filter过滤之后的图特征。

​				具体的数学：

​				傅立叶变换：$F(X)=U^T X$

​				傅立叶逆变换：$F^{-1}(\hat{X})=U\hat{X}$  ($\hat{X}$是卷积过滤后的信号，$U$的存在是正定性决定的)

​    **STEP 5**:具体的过程

​			    $$ x_{*G}g = F^{-1}(F(X)\odot F(g))$$     ($\odot$是谱空间上的一个卷积操作)

​				进一步进行定义（我就不懂为什么要这么定义，有点奇怪。。。）$g_\theta = diag(U^T g)$

​				$$x_*G g_\theta = Ug_\theta U^T x$$ （不懂。。。。）



具体的卷积图网络变形在此定义上进行变形：

1)Spectral CNN

2)ChebNet

3)CayleyNet

4)GCN 

5)AGCN [based on GCN]

6)DualGCN [based on GCN]



###  **第二类：spatial-based GNN**

对spatial-based GNN的解读：

由于传统的CNN利用的是图片的**空间相对关系邻近**进行卷积核的卷积操作（一个像素与周围八个像素邻近），所以 spatial-based GNN 在这个方法的基础上进行了延伸，想要通过对中心结点和邻居的表示的卷积来获得**中心结点表示的更新**。

从另一个角度说，spatial-based GNN 与RecGNN中信息传递/信息传播的思想一致。**空间图的卷积操作实际上做了结点信息沿着边的一种信息传递**。

比较清晰的解读，**spatial**直接借用了CNN的想法，**没有进行整个图空间的转换，具有局部性和灵活性**。

具体的spatial-based GNN 模型：

1)NN4G

2)DCNN

3)PGC-DGCNN

4)MPNN

5)GraphSage

6)GAT

7)GAAN

8)MoNet

....spatial-based GNN模型有很多，方法各不相同。

### ConvGNN的优化方向

​		训练效率的逐步提高。早起ConvGNN 的训练需要使用整图数据[full-batch training]（GCN训练），内存经常会爆炸。许多优化算法在空间复杂度和时间复杂度方面对GCN进行了时空优化。

### Spectral 和 spatial 模型的区别

spectral模型有清晰的数学背景，构造不同的滤波器能构造出不同的模型。

spatial没有数学背景，但是灵活高效，更加**受人欢迎**。

* spectral 模型要求特征值/整图训练，效率低。spatial直接通过图域的**直接作用卷积**来避免空间转换。另一个方面，spatial直接通过**图卷积局部性**，进行批量训练。
* spectral进行傅立叶变换，换句话说，**spectral变化后的频谱空间基U和原图结构是非常有关系的，一旦图结构发生了扰动，频谱空间的基扰动， 整个空间进行了扰动，只适用于固定的静态图，任何图的扰动会导致特征值的变化，频谱空间的巨变**。而spatial模型的局部性使得扰动影响不大。
* spatial模型对多样图的输入更加灵活。非均匀态的图，有向图和边输入可以**轻易地合并进入聚合函数**（啥意思？？？不懂）



## 第三段：图池化组件

什么叫做图池化？

**GNN提取特征，特征数量过多，池化做一个筛选。**

根据作用的不同，池化策略有两个类型：

1）**池化运算**：降低参数的数目和特征的数量来生成更少的特征，由此避免过拟合，计算复杂度，交换不变性（不知道这是什么？？？）

2）**readout运算**： 根据结点表示来生成图层面的表示。

之前，图粗糙化算法使用特征值分解来根据图的拓扑结构粗糙化图（不是很懂什么叫做粗糙化），但是该算法的时间复杂度非常高。

如今，max/mean/sum 池化是最简单直接的池化方法。

## 第四段：理论分析



## 三、图自编码器

**GAE**是深度神经结构，作用是将结点映射【编码】到**隐藏特征空间**，再从**隐藏特征空间**映射回【解码】图信息。

### 图自编码器有两个最主要的作用：

**1）学习一个网络的嵌入**

**2）生成一个新的图**



### 第一类作用：学习图嵌入

图嵌入是指**结点的低维向量表示**并且保留了结点的拓扑信息。

GAE学习图嵌入的过程：

1）使用编码器提取图嵌入结构

2）使用解码器来强制图嵌入保留结点的拓扑信息，通过重构PPMI和邻接矩阵的方法

（并不是很懂是什么意思。。。）

**PPMI矩阵通过图中随机游走的样本内在捕获了结点之间的共有信息**

$$PPMI_{v_1,v_2} = max(log{\frac{count(v_1,v_2) \cdot |D|}{count(v_1)count(v_2)}},0)$$

其中，$count(\cdot)$代表了结点在随机游走序列中出现的频率。

### 第二类作用：图生成

有了许多图，GAE能够通过自编码器对图的编码和解码器对图的解码来学习图的生成分布。

大多数用GAE来做的图生成都是设计用来解决分子图生成问题的。（分子图是啥？？？）

生成的图往往通过两个形式发表：**序列形式**，**全局形式**

**1）序列形式**

序列形式的算法一步一步地生成产生的结点和边。另一种生成图的方法也可以是迭代地向一个正在生长的图上增加结点和边直到满足终止条件。

**2）全局形式**

Graph Variational Autoencoder对结点和边的存在视为单独的随机变量。

**将后验概率分布$q_{\phi}(z|G)$作为编码器，将生成概率分布$p_{\theta}(z|G)$作为解码器，GraphVAE对一个KL散度进行优化**

$$L(\phi,\theta;G) = E_{q_{\phi}}(z|G)[-log_{p_\theta}{G|z}]+KL[q_{\phi}{z|G}||p(z)]$$

对上式进行优化（看不懂。。。）



















​			

​			





