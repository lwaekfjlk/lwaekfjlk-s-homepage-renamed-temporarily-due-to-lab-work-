---

title: "Non-local Neural Networks"
author_profile: true
date: 2019-11-28
tags: [Computer Vision]
mathjax: "true"
header:
    image: "/imgs/blog17.jpg"
excerpt: "Computer Vision"
---

## 本文探讨的主题：Non-local Neural Network 对long-range dependency任务的应用

## 本文做到的亮点：

**1) 提出了 non-local operations 来解决 CNN 网络中的 long-range dependencies 问题**

**2) non-local module 可以作为一种通用的模块应用在各项任务上**



## Non-Local的含义

Local这个词主要是针对感受野(receptive field)来说的。以卷积操作为例，它的感受野大小就是卷积核大小，而我们一般都选用 3$\times$ 3，5$\times$5之类的卷积核，它们只考虑局部区域，因此都是local的运算。同理，池化(Pooling)也是。相反的，non-local指的就是感受野可以很大，而不是一个局部领域。

那我们碰到过什么non-local的操作吗？有的，全连接就是non-local的，而且是global的。但是全连接带来了大量的参数，给优化带来困难。这也是深度学习(主要指卷积神经网络)近年来流行的原因，考虑局部区域，参数大大减少了，能够训得动了。

## Non-Local的意义

**长程记忆** 是深度学习的一个主要问题。

这篇文章中的Non-Local 组块在DNN中的嵌入能够抓取长程记忆。而不需要通过比较低效且难优化的多次卷积和多次recurrent操作。

我们知道，卷积层的堆叠可以增大感受野，但是如果看特定层的卷积核在原图上的感受野，它毕竟是有限的。这是local运算不能避免的。然而有些任务，它们可能需要原图上更多的信息，比如attention。如果在某些层能够引入全局的信息，就能很好地解决local操作无法看清全局的情况，为后面的层带去更丰富的信息。这是我个人的理解。

从直觉上看，非局部操作就是根据所有位置$X_j$ 的加权来推测$X_i$的response。

位置 i 可以是图像，序列或者视频上的位置。这就决定了Non-Local具有广泛应用。



## Non-Local Block的具体样子

* 首先构造 generic non-local operation【全局高斯模糊可以看作一种 specified non-local operation】

  $y_i = \frac{1}{C(x)}\sum_{\forall j}f(x_i,x_j)g(x_j)$

  xi是输入，yi是输出，i代表的是一个空间位置（图片），时间位置（音频），时空位置（视频）

  f是一个计算任意两点相似关系的函数，g是一个映射函数，将一个点映射成一个向量，可以看成是计算一个点的特征。

  C为归一化系数。

  也就是说，为了计算输出层的一个点，需要将输入的每个点都考虑一遍，而且考虑的方式很像attention：输出的某个点在原图上的attention，而mask则是相似性给出。



​		有两个特点：

​		1.和attention很像

​		2.和全连接不同，non-local公式是一个i，j位置的函数，也就是不同位置之间的函数，与全连接层中的输入数据无关，只是一种权重分配而不是一种层间前向传播关系。

* 其次关于f，g的具体取值【实验证明f和g的选择对结果准确率的影响比较小，这也是non-local适用于多种任务的原因】

  g直接看作1$\times$1卷积。

  相似性度量函数f的选择，根据全局均值和双边滤波，很自然选择高斯函数来进行相似性的度量。在原始高斯函数的度量上对将$x_i,x_j$放入嵌入空间进行高斯函数的衡量，在embedding space上进行相似性的衡量。

  由此构造出来的高斯函数为

  $$f(x_i,x_j) = e^{\theta(x_i)^T\phi(X_j)} ，其中 C(x) = \sum_{\forall j}f(x_i,x_j)$$

  [注1：其中 $\theta,\phi$为embedding的实现方式，是一种卷机操作]

  [注2:除了高斯函数，还有 **点积，Relu激活连接等方式的f 可供选择**]

  [注3:如果使用的是嵌入高斯函数作为f的话，根据C(x)和f(x,x)的定义，就相当于 self-attention 形式]

* 最后，为了能让non-local操作作为一个组件，可以直接插入任意的神经网络中，作者把non-local设计成residual block的形式，让non-local操作去学x的residual：

  $z_i = W_zy_i+x_i$

  xi输入，yi为学习到的残差，zi为总和。

* 对于具体实现

  如果按照上面的公式，用for循环实现肯定是很慢的。

  此外，如果在尺寸很大的输入上应用non-local layer，也是计算量很大的。

  大尺寸输入计算量大的解决方案是，只在高阶语义层中引入non-local layer。还可以通过对embedding( ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta%2C+%5Cphi) )的结果加pooling层来进一步地减少计算量。

  for循环求和速度慢的解决方案是，注意到f的计算可以化为矩阵运算，我们实际上可以将整个non-local化为矩阵乘法运算+卷积运算。

  

  Appendix：Self-Attention 和 Non-local的相似度很高

  ![blog16_appendix](/imgs/blog16_appendix.png)

  

## 总结

​		从实验来看，non-local block 模块对准确率的提升也并不是很大，但是具有通用性。

​		在COCO数据集上，对于目标检测任务有提升。

​		对于视频分类任务，与3D卷积(使用的是$1 \times k \times k$ 卷积核 或者$ t \times k \times k$卷积核)相比，加入non-local 模块性能都能有几个百分点的提升。





