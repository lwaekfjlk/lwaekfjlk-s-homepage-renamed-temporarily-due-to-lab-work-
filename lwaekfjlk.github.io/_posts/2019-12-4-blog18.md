---

title: "Sequence to Sequence Learning with Neural Network"
author_profile: true
date: 2019-12-04
tags: [Natural Language Processing]
mathjax: "true"
header:
    image: "/imgs/blog18.png"
excerpt: "Natural Language Processing"
---

## 本文探讨的主题：Seq2Seq模型

## 本文的关键想法：

1）不定长序列输入输出情况的普适性模型

2）用encoder-decoder模型进行序列的输入和输出

3）用固定长度的vector通过RNN实现句子的表示嵌入

4）在机器翻译（MT）NLP任务中实现了首个全神经翻译器



#Seq2Seq模型

DNN的使用场景限制于输入和输出都为**固定长度的向量形式**。

序列输入输出的长度都是未知且不固定的，所以不可以使用DNN。

Seq2Seq使用了两个LSTM作为encoder-decoder模型的两个部分。

第一个LSTM是修改过的LSTM，并没有传统LSTM的输出，而是将input的其中一个表示向量在世间变化更新迭代之后的值作为最后的输出。将输入的序列【经过word2id和embedding预处理的单词，并不是word2vec而是接近one-hot的一种方法】倒序处理【输入的序列是将ABC映射为abc，但是在处理过程中使用的将CBA映射为abc，构造出了short-term dependency】并随着序列的输入在LSTM中进行定长表示向量【v】的学习。

第二个LSTM就是真正的LSTM，和原版的LSTM一样，以序列形式输出。

###第一个LSTM 

input：初始化表示向量【v】，每一时刻的序列输入【x，y，z】

output：含有序列语义信息的sentence embedding向量【v】，并不是LSTM传统的feedforward输出而是一个输入的学习结果输出

### 第二个LSTM

input：学到的sequence embedding【v】和input sequence 结束的标志/或者output sequence 开始的标志【两个二选一，其实是一个东西】，之后的input就是每次更新得到的【v】和正式的输出序列的循环作为input

output：是一个传统的LSTM模型的输出，经过softmax处理之后输出一个概率分布，根据语料库概率分布的最大值决定当前输出的是哪一个，当输出的是<EOS>【end of sequence】代表输出序列结束，那么说明这个模型的序列输出结束



我想到的一个巧妙比喻来理解seq2seq：

思想类似于**化学反应官能团的转换，找到一个能态高的中间体产物之后再进行转换得到最终的产物**。



## Seq2Seq的训练

训练目标：

最大化翻译结果是ground truth 的log对数概率

$$\frac{1}{|S|} \sum_{(T,S)\in S}logp(T|S)$$

$$\hat{T} = arg max_{T} p(T|S)$$

Loss Function：

第二个LSTM概率分布和ground truth 之间的差距作为loss function的内容，学习到的表示向量仅仅代表一个中间产物和模型的训练并没有关系



## Seq2Seq中的trick

* LSTM使用的是深层LSTM,包含了4个hidden layer，在封面图片中layer指的是想上方传播的一个一个layer。

  而一个LSTM模块在图中的左右方向有一个一个的block通过RNN的连接方式进行连接，每个block内部还有很多的cell，一个block共用一组LSTM的input/output/forget 门

  ![blog18_3](/imgs/blog18_3.png)

  ![blog18_1](/imgs/blog18_1.png)

* 在做Translation Prediction的时候使用的是beam search decoder，相比传统输出<EOS>才终止的方法来说这个方法更加地优秀【beam search中每步保留的可能序列个数叫做beam size，beam size对结果的影响也不是很大】

  ![blog18_2](/imgs/blog18_2.png)

* 源序列的反向输入

  成功构造了short-term dependency，实验证明，由于某种不知道的原因，这样效果超好



## Seq2Seq的效果

* 作为第一个全神经翻译机，Seq2seq模型的效果部分优于STM模型【Statistic Translation Model】

* seq2seq的预测序列在STM中对top-1000 rank 预测序列的替换能够有效地增强STM的模型准确度
* Seq2seq机器翻译的效果是根据**BLEU**方法衡量的分数
* seq2seq序列学习出来的句子表示向量【v】对单词的**顺序**敏感，对**句子中的主动格变成被动格**非常不敏感，很有趣！
* 原始seq2seq性能瓶颈主要有两个因素，其一是句子表示向量是 **定长** 的，其二是 input 和 output 端的语料库是有限的，在这个论文中是160000，和 80000，语料库的有限决定了**训练的句子对的有限**，所以效果不好，可以改进








