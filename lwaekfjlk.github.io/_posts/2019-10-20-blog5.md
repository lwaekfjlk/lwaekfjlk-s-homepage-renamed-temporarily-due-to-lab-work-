---

title: "BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding"
author_profile: true
date: 2019-10-20
tags: [Natural Language Processing]
mathjax: "true"
header:
    image: "/imgs/blog5.jpg"
excerpt: "Natural Language Processing"
---

## 本文探讨的主题：BERT模型

## 本文想要解决的问题：想寻找全新的预训练通用语言模型（language representation model）			



##Language Representation Model

在BERT之前对于Language Representation Model的研究已经有很多。而BERT也仅仅是一种较为state-of-the-art和效果特好的Language Representation Model。

直白地说，Language Representation Model的目标就是找到一个无监督预训练模型，使得可以下放到下流的具体任务。

深度学习四大要素：1.训练数据 2.模型 3.算力 4.应用，从四个要素的角度来看待这个问题：

* Language Representation Model在训练数据方面采用的是无标签数据。

  在transformer模型中，需要训练的模型参数分为三类：

  a. L表示层数（transformer块的层数） 

  b.H表示隐藏大小

  c.A表示self-attention heads的数量

  必须用非监督学习的算法来进行处理。

  通用模型中存在着大量的参数，必须使用海量的数据来进行训练。

  标签数据显然是太不现实的想法。

* Language Representation Model在模型方面有单向模型，有双向模型。

  BERT中指出单向模型使用的是单向Transformer，在词语级训练中单项Transformer劣势很大，对话系统不能仅仅从一个方向进行编码因为对话系统中两边的信息都很重要，本来句子就很短，只接受一边怎么可以呢。

  BERT使用的双向Transformer，所谓的双向并不是跑两遍，而是提出一种新的**预训练目标：遮蔽语言模型**来克服单向Transformer的局限性。通过随机遮蔽一些token来预测原始词汇，训练目标融合了左右两侧的语境。

* Language Representation Model在算力方面，由于使用的是Transformer模型，在并行计算方面性能大大加强。

* Language Representation Model在应用方面有两种下放下游的方式，feature-based和fine-tuning。

  feature-based 方法在具体的结构中包含预训练的表示来作为额外特征。

  fine-tuning 方法仅仅通过转移所有的参数到下游具体任务来进行实现，优势是不需要学习很多新的参数。



## BERT具体模型

BERT的全称是Bidirectional Encoder Representation from Transformer

BERT是一种全新的language representaiton model（在文中指通用于不同情况的语言模型）。区别于其他language representation model之处体现在BERT预训练时使用的模型是Deep Bidirectional Transformer 模型。其他部分比较类似。

（预测方法是deep bidirection，主要模型是transformer模型）。

**1）BERT的大体架构**

* **在BERT的具体架构中分为两大块，pre-training和fine-tuning**

  在pre-training阶段，模型使用无标签的数据在不同的预训练任务上进行训练。

  在fine-tuning阶段，模型预先设置的参数是预训练的参数。之后根据不同的任务使用下游任务的有标签数据进行fine-tune。

  使用BERT的一个非常优越之处就在于预训练模型和下游任务的区别很小，很容易进行训练。

* **BERT的输入嵌入比较困难，通用性决定了要输入比较全面**

  由于BERT需要适应许多不同的下游任务，所以输入必须能够使用一个**单词序列**来表示**单一句子和多个句子**

  具体地说，这个输入就有点难度，所以来说，**BERT的输入是单词嵌入（代表这是哪个单词的词嵌入），序列位置信息嵌入（代表这是第几个单词），语段嵌入（代表这是第几个句子）的加和**。

**2）BERT的核心创新**

**BERT的训练过程是先MLM训练再NSP训练**

​	与Peters et al. (2018) 和 Radford et al. (2018)不同，论文不使用传统的从左到右或从右到左的语言模型来预训练BERT。相反，使用两个新的**无监督预测任务**对BERT进行预训练。

​	其一是**Masked LM**，用人话说是预测猜猜看遮住单词是啥，结合上下文预测。（从直觉上说，双向比单向好很多，尤其是在词语级别的下游任务上，所以要使用这个训练任务）

​	这个模型的核心是聚焦机制，对于一个语句，可以同时启用多个聚焦点，而不必局限于从前往后的，或者从后往前的，序列串行处理。

​	其二是**NSP**，用人话说是猜猜看下一句话是啥。（因为下游任务有问答和自然语言推理，所以上游的BERT通用模型必须要引入这个目标）

*引入了这两个无监督预测任务，和传统的单向语言模型就完全不一样了，体现在分发到下游任务时性能变得强非常多。*

**3）BERT的fine-tuning下放**

fine-tuning很直接因为transformer的self-attention机制允许BERT对许多下游任务建模。BERT因为是双向的，所以在词级别和句子级别都下放效果非常好。

