---

title: "Efficient Estimation of Word Representations in Vector Space"
author_profile: true
date: 2019-10-26
tags: [Natural Language Processing, word2vec]
mathjax: "true"
header:
    image: "/imgs/blog10.png"
excerpt: "Natural Language Processing, word2vec"
---

## 本文探讨的主题：word2vec的Mikolov原文

## 本文想要解决的问题：指出word2vec的具体优越性

此篇是Mikolov的原文。比较简略地阐述了word2vec的模型，但是比较完整地交代了word2vec 与其他模型之间的关系并且清楚地证明了word2vec 模型的优越性。

## word2vec的优越性

提出了CBOW和Skip-Gram 两种在很大的数据集上计算连续的词向量的方法。

这些词向量的质量通过word similarity task来进行比较。

（**比较语法和语义相似性，其中语法相似性指big 和 bigger ，apparent和apparently，而语义相似性指Athens 和 Greece，brother和sister**）

实验发现在更小的计算代价下word2vec相比其他模型获得了极大的进步。

**word2vec的优越点：**

1. word2vec是当时唯一一个能在几十亿个单词的训练集上在几百万个单词组成的语料库训练几百万个单词的词向量的模型，并且词向量维度保持在50-100。使用分布式计算甚至可以将CBOW和Skip-Gram放在亿级别的语料库上训练。

   这个成立的原因是因为word2vec模型全是线性的，所以计算复杂度比**前馈神经网络**比**循环神经网络**低很多**（因为这些神经网络结构中除了反向传播以外都有非线性的激活函数，增加了计算复杂度）**

   word2vec证明了可以在大规模数据上通过简单结构训练出质量良好的词向量。

2. word2vec模型训练出的词向量拥有**多重相似性**。名词结尾**相似**或者**相同**的单词有相似词向量，在一个名词周围能找到结尾相似的单词【语法方面】

3. 语义相同的词会有相似词向量，king – queen = man - woman实验在word2vec下的词向量中成立【语义方面】

4. 通过新模型的发明，保留了单词间的线性规律性【线性规律性就是单词之间这种 king – queen = man - woman的线性加减关系】，证明了单词的线性规律性能够在word2vec中很准确地学习到

## 判断word embedding 良好与否的判断标准

当前绝大部分工作（比如以各种方式改进word embedding）都是依赖wordsim353等词汇相似性数据集进行相关性度量，并以之作为评价word embedding质量的标准。然而，这种基于similarity的评价方式对训练数据大小、领域、来源以及词表的选择非常敏感。而且数据集太小，往往并不能充分说明问题。

而由于NLP是AI中任务导向性非常明确的一个学科分支。

所以应该以word embedding对于实际任务的收益为评价标准。包括词汇类比任务（所谓的analogy task，如king – queen = man - woman）以及NLP中常见的应用任务，比如命名实体识别（NER），句法分析（parsing）等。

把word embedding的评价标准下放到具体任务，才更有比较embedding 好不好的意义。如果脱离了实际意义，那么就很难将谁的word embedding好，谁的word embedding 差。

**在word2vec的原文中，Mikolov定义了一个非常全面的测试集，包括5种语义问题，9种语法问题。从一个比较全面的角度证明了：**

**word2vec是对于NLP中语法和语义任务都是有普遍价值的，是一种非常有效且有普适性的一种word embedding方法。**



## 词向量生成模型计算复杂度对比

许多模型都能够生成单词的向量表现形式，但他们的复杂度各不相同。

这些生成词向量的模型分为两类：

1. LSA和LDA模型，这些模型不包含神经网络（LDA在大数据集上运算量大，LSA在保留线性规律性方面效果不及word2vec）

2. 含神经网络的模型，前馈神经网络和循环神经网络和log-线性模型（log-线性模型包含CBOW和Skip-Gram）

以下主要比较含神经网络的计算复杂度：

1. 首先给出模型计算复杂度的定义：

   **通过充分训练模型所需的参数个数作为衡量模型计算复杂度的指标**

2. $O = E \times T \times Q$

   此式作为衡量计算复杂度标准。E是训练epoch，T是训练集单词个数，Q是每个模型的具体参数

3. 在前馈神经网络中（NNLM）：

   $Q = N \times D + N \times D \times H + H \times V$

   前馈神经网络包含四个层的计算：输入，投影（把离散的输入变成连续的向量），隐藏，输出层

   1）$N \times D$ 

   把输入层传递到到投影层。（投影层是N个单词的向量表示组合的矩阵，一个单词的向量表示有D维度，$N \times D$ 是投影层的维度）（例如**把好多个 one-hot vector 投影成一个非常长的向量**）

   2）$N \times D \times H$

   从投影层传递到隐藏层，由于投影层内的数据比较稠密，所以这部分是**前馈神经网络计算量最大的地方**，把一个$N \times D$维度的向量投影到$H$维的H个$H$个神经元中。

   3）$H \times V$

   把 $H$ 维的隐藏层元素投影到$V$维的输出层所要进行的运算。（可以通过分层softmax 的方法加速）

4. 在循环神经网络中（RNNLM）：

   $Q = H \times H + H \times V$

   循环神经网络修改了前馈神经网络的几个弊端，比如说 N 可以不确定，用序列化输入。

   使用序列化输入的优点就是没有了projection layer（投影层）计算复杂度被降低了,只有输入，隐藏，输出层，但是隐藏层序列间有循环矩阵进行相互连接，hidden layer所包含的信息才能通过循环矩阵的形式沿着序列进一步传递。

   1）$H \times H$

   词向量维度 $D$ 和隐藏层 $H$ 是同样维度的。所以input layer 到hidden layer的计算量是$H \times H$。

   2）$H \times V$

   从hidden layer 到output layer 的计算量如上，因为 隐藏层是$H$维的，而输出的语料库维度是 $V$维的。

5. Log-线性复杂度的模型：

   大部分模型的计算复杂度都来源于非线性的隐藏层。但是作者决定放弃拟合性好的非线性神经网络结构来追求速度。

   **通过两步训练，先使用word2vec的线性简单模型进行初步训练，再通过 N-gram NNLM前馈神经网络进一步训练word2vec模型得出的词向量。这样效果比较好。**

   

   #### 5.1. CBOW

   和前馈神经网络一样，但是**非线性hidden layer** 被移除了。

   

   这样剩下来的只有 input - projection -output 三层。

   叫 bag-of-word 是因为词的顺序不影响结果，和词袋模型一样，但是学习的又是连续的向量表示，和标准词袋模型又不一样，所以叫做 continous bag-of-word模型。

   CBOW使用了未来的单词，不仅仅局限于当前的单词，未来单词和当前单词一起作为输入 ，所以他有好几个单词作为输入但只有一个输出。

   所以计算复杂度为：

   ​		$Q = N \times D+D\times log_2{V}$

   是input layer 到 projection layer的操作。

   是projection layer 到 output layer的操作，但是通过 分层 soft Max技术， 利用二叉树的结构 把 V 词计算 成功取对数，降低了复杂度

   #### 5.2.Continuous Skip-gram

   和CBOW一样舍弃了前馈神经网络的hidden layer。

   输入中心词，输出一个range 内的单词，这个范围range越大，最后产生的word vector 那个中心词的词嵌入越好，但是计算复杂度就越大。既然比较边缘的词和当前词没什么关系，就通过少采样边缘词的方法来减少复杂度。

   $Q= C \times (D +D \times log_{2}V)$

   C代表了那个代表了选择的R的最大值。

   选择R个当前词之前的单词和R个当前词之后的单词作为正确的标签。这就让我门对2*R个1-C的单词分别作为input的current word，每个current word 产生会有 2 * R 个结果。

   所以  代表了input 到 projection 的运算数。

   代表了hidden 到 output layer的运算数【含分层 softmax 的log 优化】。

   

6. 并行训练【提高word2vec训练速度的方法】

   word2vec在超大训练集上的训练依赖于 大型分布式框架 *DistBelief*，在这个框架上进行并行计算能够大规模提高速度，把前馈神经网络模型和CBOW或者Skip-Gram 放在上面跑能够提高训练速度。word2vec 模型评估

7. 增加词向量维度 和训练集大小【提高word2vec准确率的方法】

   同时增加词向量维度 和 训练集大小，效果比较好。不然会有削减增加效果的一点副作用。

8. 提供多个 relationship 参与训练 【relationship 类似于 France-Paris，big-bigger】【提高word2vec准确率的方法】

   这样可以使得在king – queen = man - woman相似性检测中取得更好的准确率

## word2vec 模型评估方法

一般的word embedding 评估都是把图画出来， 看周围的词凭借直觉来说是不是类似的。这样做similarity test不是很好。

但是word2vec 评估使用的是词向量代数计算。

“什么是 smller的相似单词，如果big 和bigger相似。”

用X = vector("biggest") - vector("big") +vector("small")

**相当于给相似性定了一个标准，依据一个相似性标准来进行相似性检测，用这种relationship来检查similarity 比较准确而具体，明显优于之前看附近的点凭借直觉检测要好。**

这样的评判方法可以回答许多单词之间模糊的语义问题。