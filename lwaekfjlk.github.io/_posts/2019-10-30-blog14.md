---

title: "POPQORN：Quantifying Robustness of Recurrent Neural Network"
author_profile: true
date: 2019-10-30
tags: [Rucurrent Neural Network,AI Security]
mathjax: "true"
header:
    image: "/imgs/blog14.png"
excerpt: "Rucurrent Neural Network,AI Security"
---

## 本文探讨的主题：POPQORN(propagated-output quantified robustness for RNNs)

## 本文想要解决的问题：如何定量地衡量RNN的鲁棒性

##神经网络稳定性的定量衡量

最近的研究充分证明了 **对抗样例** 存在于各种神经网络中并且在他们的应用中存在。

【对抗样例指一种人来说看上去很像猫的图片被神经网络误分类成为了狗，就是使得神经网络在分类任务中误分类的样例，把原样本修改一点点对人眼基本没区别的东西，就使得神经网络结果原来是对的，修改后变成错的，这样的修改后的原样本就叫做对抗样例】

对神经网络稳定性的衡量主要有两种思路：

* 基于攻击的方法

  通过设计**强大的对抗攻击算法**来攻击神经网络，通过成功生成的对抗样本和原样本之间的差异性来衡量神经网络的稳定性

* 基于验证的方法

  想要找到疑似攻击方法的扰动的最小值，框定可能形成对抗样例的扰动的下界，通过这个下界来衡量神经网络的稳定性【扰动下界的意义：比我给出的扰动下界还要小的攻击一定是会失败的】

## POPQORN的意义

POPQORN方法的重要性在于这种方法是第一个针对RNN的基于验证的定量衡量神经网络稳定性的方法。

之前基于验证的方法都是针对前馈神经网络结构的。

衡量RNN稳定性的困难点：

* LSTM和GRU形式中的门结构增加了交叉非线性性，使得对RNN稳定性的衡量变得困难
* 基于验证的方法往往假设所有的输入都在input layer 同时给出，而RNN是基于时间序列的输入
* RNN的对抗样例通常是通过改变几个单词形成的，衡量操作一个单词的困难程度很重要

POPQORN的优点：

* 新颖，第一个针对验证机制的RNN处理方法
* 有效，能够有效处理LSTM和GRU结构中的交叉非线性关系
* 通用，适用于各个领域的RNN应用

## POPQORN的线性夹逼

把非线性激活函数使用线性函数限制上下界，把线性上下界通过反向传播的方式传播到最开始的第一层，找到衡量鲁棒性的上下界。使得确定低于下界的攻击都是无效攻击。

RNN在受到了 $l_p$ 球的扰动,扰动范围是 $\epsilon$ 的时候，RNN的输出可以通过两个线性函数夹出上下界。

而输出的线性函数相夹是通过激活函数【sigmoid，ReLu】的线性夹逼，隐藏层中的交叉非线性关系的线性夹逼，最终得到了RNN输出的线性夹逼。

* **在双层vanilla RNN模型的线性夹逼中【双层代表的是序列长度为a0->a1->a2，也就是序列长度为2】：**

  这个用来夹逼的线性函数被要求是是单变量的。

* **在单层LSTM模型的线性夹逼中【单层代表的是序列长度是a0,c0->a1,也就是序列长度为1】**：

  有两个不同的非线性函数，$\sigma(V)Z$ 和 $\sigma(V)tanh(Z)$ 都是双变量的，所以我们使用的是平面而不是单变量的线来进行夹逼。

以上以特例的形式数学推导了是可以线性夹逼的。将RNN序列长度【此指层数放开】，拓展到一般的情况是一样使用POPQORN算法的。

所以通过数学证明了POPQORN算法的线性夹逼思想使用与vanilla RNN，LSTM和GRU模型。

## POPQORN的鲁棒性定量分析

目的：想要通过多个序列i，j，k 输入之后的分析，找到最大的可能下界。

**STEP1**

给定一个干扰范围 $\epsilon$ ,对于一个特定的序列j，先计算序列j的上下界

**STEP2**

如果存在另一个序列 i，序列 i 的上界 大于 序列 j 的上界【满足这个条件则说明，i 和 j 的由上下界确定的波动空间是有公共部分的，说明波动太大了，每个序列的波动范围没有交叉处，就说明这个对抗足够小】，那么 $\epsilon$ 不是对抗干扰的下界，通过二分递减的方法来进行搜索。

不然的话，不存在另一个序列 i，序列 i 的上界 大于 序列 j 的上界， $\epsilon$ 是对抗干扰的下界，但不知道是不是下确界。我们通过二分递增的方法来进行搜索。

**STEP3**

变化小于阈值的时候，我们认为阈值找到了。稳定性的下确界找到了。所以就可以定量衡量出RNN的鲁棒性。









