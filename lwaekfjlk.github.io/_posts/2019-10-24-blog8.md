---
title: "word2vec Parameter Learning Explained"
author_profile: true
date: 2019-10-24
tags: [Natural Language Processing, word2vec]
mathjax: "true"
header:
    image: "/imgs/blog8.jpg"
excerpt: "Natural Language Processing, word2vec"
---

## 本文探讨的主题：word2vec的理解

## 本文想要解决的问题：从直观的角度理清Mikolov文中的思路

在研究Mikolov的word2vec两篇原始论文时，遇到了比较大的理解困难。于是决定先进行对原始论文进行解读的相关论文开始看起。由此选择此篇于网上评价极高的解释性论文进行学习。兹此予以记录。

## word2vec

NLP从处理细粒度的角度来看，word2vec处理的事最为细腻的词语级别语言。

NLP中的词语是符号形式的，通过词嵌入的方法转换成数值形式的才能使得**神经网络，SVM等只接受数值型输入的模型结构**得到输入。

把NLP的语言处理模型抽象化为：

$$f(x) \rightarrow y$$

把 x 看作句子中的一个词语，把 y 看作词语的上下文，那么 f 就是一个语言模型，模型作用就是判断**（x,y）这个样本是否是人话，说不说地通**

**Word2vec**就是这样一个语言模型，但它的最终目的不是把 f 训练地非常完美，而是关心模型训练后得到的模型参数也就是对应的两个系数矩阵 W 和 W’，将 W 的行向量 和 W’的列向量作为 输入 和 输出 的向量化表示（词向量）

**Word2vec**的模型的优越性在于它在变换的时候只进行了线性变换。在隐藏层并没有进行非线性变换而是直接把隐藏层中的向量传递到了下一层，这使得word2vec模型显得较为优越，计算量降低。

## Skip-gram和CBOW（continuous bag-of-word）模型

* 把一个词语（中心词语）作为输入，预测它周围的上下文（上下文中的C个词语），这个模型叫做【Skip-gram】

* 把一个词语的上下文（上下文中的C个词语）作为输入，预测这个词语本身，则是【CBOW】

  （CBOW的思路和BERT中的MLM任务有些相似）

  （CBOW的命名原因是其一，使用词袋模型，认为一篇文章是由一大袋的单词组成的，单词不考虑顺序；其二，由于学习的是词向量，词向量的每一个元素都是连续的，词向量是连续可微的，所以是continuous）

* 两者均为非监督学习模型

## CBOW模型的 one-word context 形式

当 X 输入是一个词语，Y 输入也是一个词语的时候可以认为是两个模型的一种特殊形式（**模型任务特殊化为用当前词X去预测下一个词Y**），这种模型也叫做simple CBOW模型结构。

###模型结构

1）在此simple CBOW模型中，由三层构成 input -- hidden -- output。三层之间是完全连接的。

2）input层使用的是one-hot编码向量（这样的话整个input层的单元个数V就是词语库的个数）。

3）input层到hidden层使用W系数矩阵进行变换，这使得one-hot中的那个 “1” 通过矩阵运算让对应的 行向量 变到了隐藏层中。（hidden layer可以认为是部分W系数矩阵的复制）

通过这样一种变换，把原来 V 长度的 one-hot 词向量表示就转换成了 N 长度的词向量表示（N << V的特点决定了这样一个矩阵变换导致的是降维操作，相当于对one-hot 词向量进行了一次压缩这样就相当于把wi 向量找到了输入向量表达vwi）

4）从input层到output层分成两步，首先隐藏层向量和 W’ 矩阵的第i列列向量相乘得到output layer中的第i个元素，这个元素就是对应one-hot向量中第i个元素为1的向量对应的单词的score。

那么得到了score，我们就可以进行下一步。通过softmax层将score转换成一个在 0 到 1 之间的数。这个数就是最终的结果，这个数的含义就是**根据前一个数X得出的后一个数Y的可能的概率**。

（比如说，X是hello，那么根据当前词预测下一个词Y是greate的概率就是P(great|hello),假设greate是语料库中的第一个元素[1,0]，hello是预料库中的第二个元素[0,1]，那么Y中的第一个元素的值对应的就是这个概率。）

（从语言模型中训练出来的W矩阵的第i行向量vw和W’矩阵的第i列向量vw’分别就是语料库中one-hot encoder 第 i 个元素为 “1” 的单词的输入向量（input vector ）表示和输出向量（output vector）表示）

5）hidden->output 层的参数更新就是用max p(WO|WI) = E作为代价函数来进行通过SGD进行参数更新

​      input->hidden 层的参数更新也是建立在求导的基础上进行运算。

 （从直观的角度来说，当我们迭代更新模型参数是通过训练语料库生成的目标文本单词对（X，Y）时，每个单词对向量的效果会累积。我们可以想象w的**输出向量** w的共存邻居单词的**输入向量**被 “拉扯”，而w的**输入向量**被许多的**输出向量**“拉扯”。这种感觉不是“重力”的感觉吗？两个球在引力的作用下相互拉扯，最后趋于一个稳定的状态。当到达一个稳定的状态之后，input vector 和 output vector 之间的相对位置就确定了，这样就逐渐趋于稳定，这样就迭代计算完成）

（迭代的收敛性和神经网络一样，只能收敛于局部最优解而不是全剧最优解）

以上就是simple CBOW模型的主要步骤和每个步骤的具体含义。



## CBOW模型的Multi-word context形式（全称为continuous bag-of-word，因为学习的是词的连续可微向量表达）

有了simple CBOW模型的具体解释，一般的CBOW模型只要通过不同点进行寻找就可以了。

**CBOW的模型任务：**

根据上下文多个词预测中心词是什么。具体地说，你知道了上下文存在着 printf ， hello ，return ， 0 要预测 中间被mask住的一个词world。根据CBOW模型输出的Y的可能性语料库向量，选择一个可能性最大的来进行预测。

**一般CBOW和simple CBOW的模型区别**：

1）input-hidden层不一样，所有的C个上下文词更新的系数矩阵都是相同的Wv*n。但是hidden layer只有一个。怎么办呢？将所有w1，w2，w3...的input vector 取平均值作为hidden layer的值，这样从直觉的角度来说就相当于结合了上下文进行文本分析。

2）更新的时候cost function -E = log p(wO|wI,1,wI,2...,wI,c)

这点有些区别，其他都和simple CBOW差不多。



## Skip-gram 模型（C个输出向量是相同的，因为通过的是同一个h 同一个W）（全称为continuous skip-gram，因为学习的是词的连续可微向量表达）

一对多的输入和输出，和CBOW模型时相反的。

**Skip-gram的模型任务**：

根据一个中心词来预测周围上下文中的C个位置分别可能是什么单词。**相当于对一个单词进行一个分类。**

知道了中间词是hello，想要预测语料库中所有的词在hello附近最大的那几个词是什么，就是最可能和hello结合的词有什么，比如说world，everyone，good，morning在hello左边第一个，左边第二个，左边第三个，右边第一个，右边第二个的概率最大。

**输出 C个向量，就是说输出的是C个各不相同的概率分布。每个概率分布对应着上下文的一个位置。**

**Skip-gram模型和CBOW模型的区别**

Skip-gram 一对多，CBOW多对一，这样相似但是相反的思路肯定会出现不一样的地方。

* 和CBOW一样，output layer 使用的系数矩阵W’都是相同的。（**注意：hidden layer的结果是N维的矢量，通过同一个矩阵W’转换成C个相同的V维向量**）

* cost function -E 改变为了 log p(wO,1,wO,2...|wI),（注意：**C个位置的不同就体现在了cost function中是含真实的output word 的，真实的output word 作为cost function的一部分，因为wO,1,wO,2...是真实的output word**）其他部分在迭代的时候主要思想都是相同的。也是W和W’两个矩阵进行参数更新的时候相互拉扯最终达到平衡状态。



## **Word2vec的计算优化trick**

其实，优化方法并不是 Word2vec 的精髓，上面提到的更新迭代和网络模型结构才是精髓，优化方法只是它的训练技巧，但也不是它独有的训练技巧。 Hierarchical softmax 只是 softmax 的一种近似形式，而 negative sampling 也是从其他方法借鉴而来。

每个单词都在word2vec 模型中对应着两个表示，vw和vw’。学习input vector 非常容易，但是学习output vector 代价非常大，vw’的更新需要建立在迭代的基础上进行，而且一迭代就要对每个语料库中的单词进行迭代，得出各种东西作为迭代的材料。

对每一个训练周期都这么训练会非常爆炸，特别是当语料库是超大级别的时候。

**由此，一个非常naive的思想就是限制每个训练周期要迭代更新的output vector的个数**

根据这个思想，建立起了Hierarchical softmax和negative sampling优化方法来选择更新部分output vector 具体选择那些output vector 进行更新。

### Hierarchical Softmax（分层softmax）

说白了就是一种快速计算softmax的方法。从直觉来说，就是使用二叉树的结构来表示所有语料库中的词语【叶】和内部选择关系【非叶】，将一个O(N)的问题转换成了一个O(logN)的问题。

**二叉树的组成：**

**叶子：单词**（对应着一个概率）

**内部结点：预测器**（对应着一个向量，判断随机游走的方向）

在Hierarchical softmax结构中，没有词语的output vector 表示，只有树内部结点对应的output vector，那个单词作为最终的输出单词的概率由从叶子到根的path上的inner points 对应的向量的组合作用决定。

每个内部结点都相当于一个预测器，决定了在这个节点应该向左孩子走还是向右孩子走。如果这个节点对应的向量对应的随机游走最后输出的概率和ground truth 非常一致，那么那些内部结点的对应向量就不会发生很大的改变，如果和ground truth 差距非常大，那么内部结点对应的向量就会发生很大的更新。最终使得这些预测器的叠加能够指引随机游走到较为正确的方向从而输出最终的判断。【这种更新公式，在CBOW和Skip-gram中均为适用，所以可以作为一种优化策略来加速计算，降低计算复杂度】

### negative sampling（Skip-Gram 模型和CBOW模型均能使用的技巧）

这方法的思路非常直接。既然output vector 更新的数量要进行减少，那么就暴力地只更新一部分向量。本质就是选择预测样本总量中的一个子集。

显然，想要对输出的概率分布进行优化，那个本来那个位置对应的单词（ground truth，positive sample）应该一直得到更新。他对应的output vector 是我们判断的标准，必须得到更新。

在选取其他样本中，我们应该要选取一定数量的**负样本**（在小数据集中负样本一般选取5-20个，在大数据集中负样本一般选择2-5个），也就是说负样本是输出预测 Y 正确的可能性较低的样本，起到了限制ground truth  positive sample 过拟合的作用。

在负样本的选择中，并不要采用构造**良定义的多项式分布的负样本集合来选取负样本**，而是通过一种“一元模型分布”来选择负样本。通过出现频次的3/4次方给每个单词赋权重，与总的所有权重相除得到概率来确定一个单词被确定为负样本的概率。3/4是一个经验的超参值，说明这个分布可以任意选取，这个分布就被称为**“噪声分布”**。

负样本作为 cost function E的一部分做惩罚作用，防止了positive  sample 过多的时候出现的词向量相互过于接近过拟合的情况，限制了E，将模型的训练量变成了原来的0.05%左右，极大地加快了速度。

从这个 含负样本的 cost function，我们所定义的 更新公式 和迭代公式都可以使用在 Skip-gram模型中。 