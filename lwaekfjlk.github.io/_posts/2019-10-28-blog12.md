---

title: "ProNE： Fast and Scalable Network Representation Learning"
author_profile: true
date: 2019-10-28
tags: [Natural Language Processing, network embedding]
mathjax: "true"
header:
    image: "/imgs/blog12.jpg"
excerpt: "Natural Language Processing, network embedding"
---

## 本文探讨的主题：快速图嵌入技术

## 本文想要解决的问题：构成快速且可随数据集扩展而扩展的图嵌入模型，解决大规模数据集训练慢的问题

## 表示学习与图嵌入

表示学习的目标就是在保留特定性质的同时把一个网络的结构投影到连续的空间中【embedding】。

如今做图嵌入的方法主要分为以下几种：

* 矩阵因式分解：GraRep，HOPE，NetMF，使用tSVD或者是随机tSVD理论

* 基于Skip-Gram的模型：DeepWalk，LINE，node2vec

  有研究表明Skip-Gram是一种隐式的矩阵因式分解，因此产生了NetMF模型，ProNE也是基于Skip-Gram 思想构造出的矩阵因式分解，但是ProNE构造出的矩阵是一个稀疏矩阵，导致其速度较快

* 图神经网络GNN方法中ConvGNN的spectral-based 方法

  一个方向就是，使用降维技术做基于谱方法的图嵌入，Isomap，拉普拉斯特征值映射方法就是此种方法；
  
  另一个特别重要的工作是想把图谱转换成监督或者半监督图学习，GCN就是这方面的工作

由前两种方法pre-train出的图嵌入经常送给下游的任务来进一步具体化，例如GNN作为下游任务的模型对嵌入进行再训练。

ProNE也借鉴了这种思想，将稀疏矩阵分解之后的embedding结果进行GNN 谱传播的进一步优化。

## ProNE 大体思想

ProNE模型就是这种思想的体现，把**稀疏矩阵因式分解**的结果放进ConvGNN中spectral-based的谱传播方法中进行进一步训练。

**稀疏矩阵因式分解**，学习到的图嵌入表示往往学习到的是局部结构信息。因为本身就是做的就是SVD之后进行截断，去除多余的信息，降低维度。

**谱传播**，学习到的图嵌入模型往往学习到的是全局网络性质。因为谱传播方法本身就是对整张图进行的谱空间变换，获得的会是全局聚类信息和局部平滑。

基于结合全局性质和局部性质的思想，形成了ProNE先矩阵因式分解再谱传播的方法。

ProNE 有几个非常惹人注目的优点：

* 训练非常地快，由于计算矩阵的 **稀疏** 性和**稀疏随机矩阵tSVD技术**的使用，时间复杂度得到了很大的优化,时间复杂度只有$O(k|E|+|V|d^2)$，模型计算的主要耗时点就在矩阵的计算
* 效果非常地好，在比传统方法快很多的情况下，模型准确率超过了之前的所有方法【以结点分类预测任务作为嵌入质量的衡量标准】
* 谱传播方法具有普适性，用在 ProNE 模型中的谱传播方法对于嵌入的增强在 DeepWalk，LINE，node2vec，GraGrep，HOPE中使用谱传播技术均可以非常有效地提升模型嵌入的嵌入质量【谱传播能够有效地得到全局的聚类信息和局部平滑信息】
* 具有可扩展性，ProNE的训练时间和图网络的规模成线性增长关系，训练时间和网络的稠密程度也成线性增长关系。所以ProNE模型有适用于大规模数据集的可能性，并且可以大概预测出训练时间【结点个数和时间线性正比增长，结点的嵌入维度和训练时间成正比关系，网络的稠密程度和训练时间成正比关系】
* 由于稀疏矩阵的乘积可以通过并行运算进行处理，而稀疏矩阵的处理占了ProNE模型计算的很大一部分，所以ProNE模型的**多线程并行**优化效果好，也就是说ProNE的速度可以进一步提升

 ## ProNE 具体原理【对图中的node进行embedding】

分两步对模型具体内容进行讲解：

* ###**稀疏矩阵因式分解作为快速图嵌入技术【速度快】**

  这个高效的稀疏矩阵因式分解方法技术思想来源于Harris 提出的distributional hypothesis。即认为一个单词的含义由其周围词决定，若是两个单词的周围词想接近，那么就可以认为一定程度上两个单词的含义相近。把单词换成结点，道理相同，hypothesis 仍然成立。

  

  ###**Step1：稀疏矩阵因式分解进行词嵌入的获取**

  首先，和word2vec相似，每个单词  $v_i$  有两个词向量表示， $v_i$ 和 $c_i$ 分别表示这个单词作为中间词和文中词时候的词向量形式。那么现在把单词换成结点，这样就是这篇文章中构建的思路。利用edge结构构建 “结点——内容” 模型。

  

  $$r_i^{T}c_j = ln{p_{i,j}} - ln{(\lambda P_{D,j}) }   $$         $(v_i,v_j) \in D$

  上式的大小用来衡量 $v_i$ 的中心词嵌入和 $v_j$ 的四周文本形式嵌入的**相似性**。

  其中：

  *  $p_{i,j} = A_{ij} / D_{ii}$ 代表了$(v_i,v_j)$ 单词对在 D 中的**重要程度**
  * $P_{D,j}$ 代表了负采样样本和周围文本的结点 $v_j$ 的关联程度
  * $\lambda$ 是一个正则系数

  将这个作为 $M_{i,j}$ 的值填入矩阵 M。

  如果$(v_i,v_j)$ 是训练集中的样本单词对则填入矩阵否则就将矩阵中的值设置为0。这样一来的话，M矩阵的稀疏性肯定是可以得到保证的。

  因为M的规模非常大，$|V|\times|V|$ 规模的矩阵中之后很少的单词对在traning set 中，也就是说，M矩阵中的大部分(i,j)对都不在训练集中，那么就意味着这个矩阵非常稀疏的，有值的地方就是代表着这个单词对是在训练集中的，而且那个值代表了结点对中 中心结点 和 邻近结点embedding的相似程度。

  也就是说，在这个 distributional similarity-based network embedding 模型中，我们使用的是单词和他上下文单词，结点和他相邻附近结点之间的关系来表示这个结点的信息。

  这样的话，用词语上下文的关系来给出图嵌入的**目标**就转变成了 **矩阵特征值分解**。

  使用tSVD【截断奇异值分解】对M做特征值分解：

  $M = U_{d} \sum_{d} V_{d}^{T}$

  其中下标 d 代表了对 n 维进行 d 维度的截断，进行SVD得到的强制性降维，也就是说跟觉SVD奇异值分解的情况抛弃对全局影响小的那些维度。把一个 $N \times N$ 的矩阵变为一个 $N \times D$ 矩阵。

  之后得到的R矩阵：

  $R = U \sum^{\frac{1}{2}}$

  R矩阵中的每一行代表了一个结点的embedding。

  通过先训练Loss function 再 SVD 的方法，embedding通过矩阵被初步确定。

  

  ###**Step2:利用随机tSVD进行快速embedding**优化

  对于tSVD，文中还给出了根据随机tSVD矩阵对tSVD进行优化，进一步降低了时间复杂度。



* ###**谱传播作为图嵌入增强技术【效果好】**【比单纯进行系数矩阵分解提高了几个百分点的准确度】

  ###**Step1:在embedding 增强， 图谱和图划分之间建立联系**
  
  首先是相关概念，对于$S \subseteq V$,用 $vol(S) = \sum_{u \in S}d_u$来表示图的分区，特别地，$vol(G) = \sum_{u \in V|G|}d_u$。
  
  说完了图的分区，在说一下每个 子图 的一个评判标准。
  
  $ \phi(S) = \frac{|E(S)|}{min(vol(S),vol(V-S))}$
  
  其中 E(S) 是只有一个端点在 S 中的边集， vol(S) 是 结点集合S中顶点度的总和。这个系数部分显示了一个结点集 S 的边，边“扩展”到结点集 S 之外的概率。
  
  接下来对 $\phi(S)$ 进行拓展，每个子图都有一个$\phi(S)$ ，这样把这个系数的概念扩展到整张图上就变成了Cheeger 常数
  
  Cheeger常数的概念如下：
  
  $\rho_G(k) =  min(max(\phi(S_i):S_1,S_2,...S_k \subseteq V disjoint))$
  
  这样Cheeger常数就拓展到了整个大子图上。
  
  Cheeger 常数从概念上来说衡量了整个图中任选一个结点集合S中顶点的边，这条边“拓展”到结点集合 S 外的概率，反映了把一个图划分成 k 部分的效果，**一个小的值意味着一个更好的分割效果**。
  
  拉普拉斯矩阵的特征值又可以对图的 Cheeger常数进行限定，所以就通过 Cheeger 不等式建立起了图谱和图分划的关系。
  
  $\lambda_k/2 \leqslant \rho_G(k) \leqslant O(k^2)\sqrt{\lambda_k}$
  
  Cheeger 不等式从直觉上说，意味着小的拉普拉斯特征值通过把整个网络划分成几个小部分控制了网络的全局聚类效果。
  
  这样就激发了我们在谱空间传播图嵌入嵌入来合并全局和局部网络信息到图嵌入内部。
  
  怎么传播呢？
  
  $R_d \leftarrow D^{-1}A(I_n - \widetilde{L})R_d$ 
  
  其中 $\widetilde{L} = Ug(\bigwedge)U^{-1} $是调节过的拉普拉斯矩阵，使用 g 作为谱调制器，$D^{-1}A(I_n - \widetilde{L})$是调整过的图结构。
  
  g 作为谱调制器是 $e^{-0.5[(\lambda- \mu)^2-1]^{\theta}}$,可以认为是带通滤波器。g把一定范围内的特征值放过去并减弱那个范围之外的特征值，这样就意味着$\rho_G(k) $的减弱和对应局部和全局图信息的增强。
  
  
  
  ### **Step2:使用切比雪夫拓展来提高效率**
  
  使用截断切比雪夫拓展来避免明确的特征值分解和傅立叶变换，这样就增加了速度。
  
  截断切比雪夫拓展提供了 $e^{-x\theta}$的一种近似计算快速收敛的方法，使得$R_d \leftarrow D^{-1}A(I_n - \widetilde{L})R_d$ 谱传播公式结合截断拉普拉斯变换让图嵌入增强很快地执行。
  
  为了增加原本稀疏矩阵获得的嵌入空间的正交性，我们在$R_d$ 上再次使用 SVD。
  
  总之，通过切比雪夫拓展，**谱传播**公式$R_d \leftarrow D^{-1}A(I_n - \widetilde{L})R_d$的计算速度得到了优化。
  
  
  
  
  
  
  
  
