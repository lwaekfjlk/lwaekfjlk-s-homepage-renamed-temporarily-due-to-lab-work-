---

title: "Distributed Representation of Words and Phrases and their Compositionality"
author_profile: true
date: 2019-10-27
tags: [Natural Language Processing, word2vec]
mathjax: "true"
header:
    image: "/imgs/blog11.png"
excerpt: "Natural Language Processing, word2vec"
---

## 本文探讨的主题：word2vec

## 本文想要解决的问题：Skip-Gram的具体优化方法和从 word 到 phrase 的拓展

此篇是Mikolov的原文的补充论文。

此篇论文主要讨论的点有以下几个：

1）以Skip-Gram为例讨论一下如何更好地训练word2vec模型【CBOW模型也可以使用的技巧】

2）如何处理“New York Times”此类的名词短语

3）对模型中的词向量线性性进行进一步讨论【一种额外的线性关系】 

## Skip-Gram的具体优化策略

让原版Skip-Gram 跑得更快，算得更准的方法有四个：

1）分层softmax

​		对 full softmax 的计算高效近似是分层softmax。原本神经网络的输出要计算 W 个结点，但是现在只要计算 $log_{2}{W}$个结点。

​	    分层softmax，把 W 个结点的输出作为二叉树的叶结点，每个非叶结点代表了孩子结点的相对概率，然后定义一个随机游走来把可能性分配到每个单词上。每个单词的概率由从根到叶的所有经过结点共同决定。

​		传统的softmax，每个单词的两个 input 和 output vector 都要使用，分层softmax 使用每个单词（叶结点）的一个 vector 表示和每个中间结点（非叶结点）的一个vector 表示。

​		此处使用的是 **哈夫曼树**，将出现频率最高的单词分配最短的编码来进行树的构建和训练，极大地缩短了训练速度。

​		事实证明，根据单词出现的频率进行聚集单词是神经网络语言模型训练加速的有效手段。

2）噪音对比估计【Noise Contrastive Estimation：NCE】

​	   NCE假定一个好的模型一定要能够通过logistic回归吧data从噪声中区分出来。把真正的 WO （应该要output的单词）从噪声分布$P_{n}(w)$中分离出来。

3）负采样【Negative Sampling：NEG】

​	   关于 负采样 的内容在之前的博客中有详细讨论。在此不多赘述。

​	   **负采样实际是对 NCE 在生成词向量这一具体任务时的简化。**

​	   简化体现在 NCE 要知道具体的 *噪声分布的数值概率*和*样本*，但 NEG 只需要使用样本。

4）根据出现频率子采样【Subsampling】

​	  在很大的语料库里面，“a”，“the” 一类非常常见的词提供了很少的信息，“France”一类不常见的词同样出现频率下提供了较多的信息。

​	 于是产生了自然的想法：**出现频率高的单词在大数据集训练后词向量应该不怎么变化**。

​	 **每个训练集中的单词，根据一定概率被抛弃。** 

​	$P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}} $

​	通过这么个很简单的手法

​    1）提高了不少个百分点的准确度

​	2）减少了好几倍的训练时间



在类比测试中，结果显示subsampling作为通用技术提升了准确率和训练速度。NEG和NCE和分层softmax作为具体技术，NEG训练时间最长，准确率最高。

## 用神经网络模型学习短语

许多短语的意思并不是短语每个词语意思的叠加。为了学习短语，我们找的目标是**经常在此篇文章中出现在一起，但是在其他文章中不经常一起出现的单词组**。

上述目标的数学的评价公式 $score(w_i,w_j) = \frac{count{w_i w_j}-\delta}{count(w_i)\times count(w_j)}$ [$\delta$ 用作防止太多不常用的单词形成短语的情况]

有了这个阈值就可以用来筛选判断是否是短语。

满足以上条件的这些单词“New York Times”等视作一个token来看待。这样就能在不增加语料库的情况下生成许多合理的短语。



类比测试中，短语作为token的训练结果依旧不错。

“Montreal Candadiens” - “Montreal” + “Toronto” = “Toronto Maple Leafs”是类比测试的结果。

## 额外的组合性(vector(A)+vector(B) = vector(C))

Skip-Gram 训练出来的词向量 有 king-queen = man - woman 的线性关系用来做**类比测试【analogical reasoning】**来作为判断词向量好坏的标准测试。

更有趣的是，Skip-Gram模型生成的词向量线性性还有其他形式。

 Russian + river = Moscow形式的线性关系证明：

**单词之间的相加能生成新的单词组合了两个的语义。这种相似性与之前 king - queen - man -woman 这种相似程度用距离衡量的线性性完全不同**

这种线性的性质的解释是，Moscow 与 Russian 有很大关系， river 和 Moscow 也有很大关系，这种 关系的 AND 操作【运算为+】导致了 Moscow的出现。

## 关于词向量线性性质的思考

词向量训练出来后king-queen = man - woman的线性性并不是 word2vec模型线性操作的原因。

用 RNN 训练词向量在数据集变大的时候，仍然训练出的词向量有很好的线性性，尽管他是用的非线性结构训练的。

词向量线性性和word2vec的线性运算关系并不大。