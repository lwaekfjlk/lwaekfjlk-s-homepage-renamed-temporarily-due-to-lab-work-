---

title: "Anytime Bottom-Up Rule Learning for Knowledge Graph Completion"
author_profile: true
date: 2019-10-22
tags: [Knowledge Graph]
mathjax: "true"
header:
    image: "/imgs/blog6.png"
excerpt: "Knowledge Graph"
---

## 本文探讨的主题：知识图谱补全（链路预测）的方法

## 本文想要解决的问题：通过不被看好的 rule-based 方法做出效果好的知识图谱补全方法



## 知识图谱补全（链接预测）

知识图谱补全可以分成3个子任务：（三元组三部分补全）头实体预测、尾实体预测以及关系预测。这篇文章中的补全主要指关系预测。

知识图谱的补全主要有两种方法：

* （目前研究主流）通过隐表示（latent representation）的方法实现知识图谱补全。

  具体来说，利用的是将一个知识图谱嵌入到低维向量空间（latent feature space），抽取latent representation来进行补全

* （目前研究次流）通过规则寻找方法（rule-based）学习到具体的逻辑符号表示（symbolic representation），从底到顶或者从顶到底的方法根据学习到的rule实现知识图谱的补全。

  具体来说，这个流派基于的想法是把一个样本看作是一个特殊规则的紧凑表达，这个紧凑的特殊规则可以通过泛化来获得一个正样本完整子集，子集中的元素都满足这个规则，那么就可以说找到了一个symbolic representation。

对于这两个主要思路，有一些对比：

* 通过bottom-up方法来做的优点是具有极强的可解释性，但是隐表示没有明确解释。
* rule-based的逻辑规则寻找方法被认为不能解决一个以上的**平凡规则子集**（找到满足规则的平凡子集数量只能是一个）
* rule-based方法被认为不能应用到大的数据集上
* 目前许多的研究方法结合了embedding和rule-based 这两种方法

**而rule-based 的两个问题，这篇文章证明了纯rule-based方法可以通过适当的修改得到解决，rule-based方法不能解决多个平凡子集和不能应用于大数据集的看法是错误的，能够做到效果优异，运算快存储少。**



## AnyBURL模型基于rule-based方法的改进大体概括

* 把rule-based方法中的样本的概念建立在path的概念之上。对example 定义的范围扩展

  用连接两个实体的路径（路径代表了链接两个关系实体的序列）作为特征，来预测两个实体之间的关系。

* 学习不确定的rule，不仅仅局限于学习specific rule

  具体谈一谈，就是学得的rule即使有不少负样本在里面，我们也是可以利用这种rule的。并不一定要做到全是正样本才算是rule。

* 通过重构知识图谱让rule-based方法变得更加有效

  具体说一说，意思就是知识图谱的fact都是从谓语（这里把谓语看作一种关系）和单词点（这里指KG中的entity）中组合成的。

  所以，知识图谱可以重构为一组边带有标签的path。

  在谓语关系对应的二元关系和多元关系缺失的时候把注意力集中到path上来能够有效抽取rule。（**谓语二元或多元关系主要体现在边上，此部分主要包含知识图谱的rule信息**）

  

## AnyBURL模型的优越性

* 可解释性
* 学到的rule set 可以在类似谓语场景中复用
* 不需要学习特定参数



## AnyBURL模型的具体算法

### 写在具体算法前：

* KG的内容组成：

   1）$C$ 代表Lisa，Lucy 等具体的constant，又称entity

   2）$R$ 代表 married，born等谓语，又称relation

   3）$G$ 代表${r(a,b)}$的集合，是fact的集合

* Anytime解释

  学习规则时间更多，规则更多，应用规则花费时间更多。

  这个算法在前期增长极快，后期渐渐放缓。小数据集训练十秒就效果state-of-the-art。大数据集AnyBURL模型学习10s之后的结果已经相当好了，可以在任何时刻暂停训练，再重新开始。这就是说可以训练Anytime的真正含义。

  此种特征毫无疑问地击败了**rule-based方法无法应用于大数据集的看法**。

* Bottom-Up解释

  知识图谱主要有**自顶向下**(top-down)与**自底向上**(bottom-up)两种构建方式。

  自顶向下指的是先为知识图谱定义好本体与数据模式，再将实体加入到知识库。该构建方式需要利用一些现有的结构化知识库作为其基础知识库， 例如Freebase项目就是采用这种方式，它的绝大部分数据是从维基百科中得到的。

  自底向上指的是从一些开放链接数据中提取出实体，选择其中**置信度较高**的加入到知识库，再构建顶层的本体模式。目前，大多数知识图谱都采用自底向上的方式进行构建，其中最典型就是Google的Knowledge Vault。

* Rule Learning解释 

  把example的概念建立在path上，**第一条改进**。

  找一条长度为n的path。(n-1)条path为rule的body部分，1条path为head部分。这样一个序列称为长度n-1的rule。

  $h(Y,X) \leftarrow b_1(X,A_2),...,b_n(A_n,Y)$

  由n条path组成的rule分为三种$AC_!,AC_2,C$，三种分别对应着由无环路径泛化得到的rule，由有环路径泛化得到的rule，由有环路径或者无环路径泛化得到的rule。

  **第一步**，从具体example中找到路径得到rule，$A_1,A_2,...$是较为具体的entity，$b_1,b_2,...$是较为具体的relation。

  **第二步**，对于过于具体的没有用的rule进行泛化，将$A_1,A_2,...$$b_1,b_2,...$变成具体**subset**中的通用entity和relation进行处理。

  **第三步**，对泛化后得到的rule进行置信度的判断，根据置信度来判断是否加入到知识库。（置信度定义为让**rule中的head头正确**的比例）。

  

### 具体算法：

1）循环列举所有长度的路径，从2开始枚举n，学习（n-1）长度的rule

2）对于每个长度的路径，直到一个饱和值sat达到，结束此长度路径的枚举和计算，进入下一个长度路径的枚举（其中sat是一个需要人为设定的超参，代表了sat%的采样活动在导致了已经知道的rule，说明rule已经被发现地差不多了，不再需要继续发现rule，另一个超参Q，代表着决定学到的泛化rule能否被存储，一般用置信度表示）

3）整个程序限制在ts时间内运行，先在一定时间内寻找rule，达到sat退出，再在一定时间内评估rule，达到Q进入知识库否则抛弃，如果rule和之前设置的参数Q相符合的话就进入知识库，实现从底到顶的KG构建。

### 应用学习到的rule来解决补全问题：

**学习rule处理补全问题**：rule目的是解决**r(a,?)**问题 

通过rule的应用，这个 **？**会有多个可能的选项作为答案。选择哪个作为最终的答案成为了一个问题。

对于根据rule 得出的预测样本的排序选择问题，只要根据这么一条原则就可以：

**如果预测的样本如果是K的话，所有包含Kentity的rule的置信度能不能得到最大化的提升**

只要根据这个简单而有效的原则，就可以根据对所有含该entity的rule的置信度提升作为指标来进行排序。

