---

title: "Fast and Discriminative Semantic Embedding"
author_profile: true
date: 2019-10-31
tags: [Word Embedding]
mathjax: "true"
header:
    image: "/imgs/blog15.png"
excerpt: "Word Embedding"
---

## 本文探讨的主题：Fast Term and Sentence Embedding

## 本文想要解决的问题：改进传统的 Random Projection 方法，利用RP的高速方法来进行快速词嵌入

## 目前embedding的问题

像word2vec一样的制造embedding的方法能够兼具**稠密性**和**独特性**。但是面临着计算量大与神经网络参数难找到的问题。于是这样的embedding方法就作为上游Pre-training model进行训练，再下放到具体的下游NLP任务。

但是Pre-train model又会导致无法抓取重要的具体任务对应的信息。所以要考虑一种不是Pre-training，但是能够非常快速得到非常准确的 word embedding 的方法。

那么就参考的主要思路就变成了基于统计的 **全局共现矩阵** 方法而不是计算量很大的神经网络训练方法。

基于 **全局共现矩阵** 的方法主要思想是 **降维**。把词向量从高维空间降维到低维空间。（PCA，LLE，Random Projection）

其中，我们的目标是提高速度，那么 Random Projection 非常地快，主要原因是因为不需要根据优化函数进行优化，并且是一种线性的低维投影。这两点原因决定了 Random Projection的计算速度非常快。

但是这两个线性投影和无优化的特点牺牲了embedding的质量。因为没有优化并且是线性投影，embedding的嵌入维度变得相对比较大，也就是说embedding的vector space 不是那么地compact。

## 目前的document embedding和term embedding方法

* term embedding方法主要分为两类：

  * 基于全局共现矩阵【LSA，RP】
  * 基于局部滑动窗口【FastText，word2vec】

* document embedding方法：

  也有对应的基于 DL 的方法，但是普遍需要的运算量大。

  在 document embedding中，计算平均词向量是非常迅速的，但是在计算 sentence/document embedding时给每个单词赋予比较正确的权重很困难。

## 此篇文章所提出方法的思路

#### 【对 Random Projection 方法的两步简单优化，在保证 RP高速性的前提下提高embedding的质量】

Step1:先利用Random Projection 进行快速降维嵌入。

Step2:对普通的Random Projection 方法进行优化，具体的优化方法有两个。

* 把词向量投影到与 **平均语言向量【全部词向量的平均值】** 正交的超平面上。通过此种方法能够过有效提高向量表示的独特性
* 给每个单词赋予恰当的权重。有了这个权重就可以很方便地根据向量的带权平均值计算出 document embedding。

**模型的主要优点：**

* 快，空间复杂度低
* 没有迭代优化，嵌入和初始值无关，每次运行没有不确定性
* 在训练数据中跑一轮【一层for】就得出了语义独特的单词的嵌入和权重

## 此篇文章所提出方法的具体算法

### 算法中的五个思想

* 对 Random Projection计算的加速

  $C' = C \cdot R$

  传统的RP方法根据上式进行计算$n_V \times D$ 的 $C'$ ，$C'$为语料库中所有单词的 D 维词嵌入

  进一步进行等价转换

  $C' = \sum_d C_d R$

  这样的方法对于 C 的存储空间要求过大。分析这个方法，把 C 分解为$\sum_d C_d$。这样的话，$C' = \sum_d C_d R$, $C_d$ 由 0 和 1 一起填充，代表两个单词到低有没有一起作为单词组在共现矩阵中出现过, $\sum_d$代表出现的频次。

  这样我们可以进一步对矩阵乘积进行分解。

  进一步进行近似转化

  $C' = I C_d R$

  魔法的一步是这样的：根据$C_d$的性质我们可以知道每个文件中的单词对投影的影响是相同的。所以C’与R的关系直接相加就可以了。相加的原因就在于每个单词对投影的影响是相同的，于是不用加权直接相加。

  **通过这个 Fast 技巧相当于直接把 $\sum_{d}$看成是单位矩阵**直接把 R 所有相关行的和叠加到 C’ 上去，这样就把时间复杂度降到了 O(V)。

* 对共现矩阵加权来增强鲁棒性

  那么之前的时候在处理 R 的时候并没有加权，实际上是不是很合理的一个东西。所以说这里还是要对 C 进行一个加权操作

  对共现矩阵 C 进行权重分解目的是来降低 a ，the 等高频单词的影响力，增加很少出现的单词的影响力。这个根据词频次得出的权重写为 $\frac{1+log(c_t(d))}{\sqrt{d_t}}$,使得得到的 C 的嵌入更加准确。

* 正交投影

  传统模型丢弃非常不常见的单词和非常常见的单词。【因为这些单词对模型的训练没什么作用】在我们方法中，我们对每个单词都给出了一个连续的权重，也就是说把这个单词的embedding vector投影到与平均 language vector垂直的超平面。

  经过这步投影，不重要的embedding vector部分被average vector 抵消，剩下的部分表示了单词的独特性，投影之后还需要进行一步正交化来规定单位长度。

* 词向量权重分配

  根据 average vector 的投影后，我们通过衡量和 average vector 之间的距离就可以得出 单词 v 的权重，$w_t = 1 - cos(\hat{v_t},\hat{v_a})$

* 文本嵌入

  有了文本中各个单词的词向量和对应的权重，我们就可以把所有的词向量加权求得的平均向量就是文本嵌入的文本向量。

Step1: 每个单词t初始embedding为0向量，权重也为0

Step2:得到带权的共现矩阵并

Step3:根据 average vector 进行投影

Step4:根据投影结果计算权重

Step5:根据词向量和权重来计算文本嵌入向量

## 实验结果

* 对句子相似性基准测试的实验

  相比于其他方法，本篇文章中的方法速度提升**极大**，准确率也有一定提升。

  实验得出 projection和weighting 对相似性准确率判断的提高贡献很大

* 对独立名词权重的定量衡量

  出现频率次数低的单词权重小

* 主题预测任务【预测文章的标题，给出一个文本，预测这个文本对应的标题是什么就是subject prediction task】

  FastText 和 Sent2Vec 都可以用作监督文本分类器。

  但是这两种方法给出的主题预测都是“human”和“female”之类的比较宽泛的词汇。

  此篇文章中提出的方法用来做主题预测任务的话比较地具体。

  举一个比较具体的例子：

  * 真正的标题是 “丙烯酸树脂”
  * 本篇文章模型得出的标题预测是“眼睛内部，晶状体植入术”
  * FastText 和 Sent2Vec 模型得出的标题是“Humans” 和 “Humans”

  




