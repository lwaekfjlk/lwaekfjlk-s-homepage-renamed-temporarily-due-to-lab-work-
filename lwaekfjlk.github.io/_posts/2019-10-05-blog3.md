---
title: "Densely Connected Convolutional Networks"
author_profile: true
date: 2019-10-05
tags: [Convolutional Neural Network]
mathjax: "true"
header:
    image: "/imgs/blog3.jpg"
excerpt: "Convolutional Neural Network"
---

## 本文探讨的主题：稠密连接的卷积神经网络

## 本文想要解决的问题： 如何使得深层的卷积神经网络降低梯度消失的影响

# DenseNet

卷积神经网络如果在输出输入出增加层间连接能够使网络更深更有效地训练。

基于此想法，DenseNet出现了。

传统的CNN的L层结构是需要有L个连接。在DenseNet中，L层结构对应的是$L(L+1)/2$次连接。DenseNet干脆把每层都和其他层连了起来。在每层网络中，之前层所有的特征图都作为input输入当前的layer。稠密连接的“稠密”性体现在每一个连接层都与前面的层全部连接。

总体来说，DenseNet的这种稠密连接带来的好处有：

* 减轻梯度消失的影响
* 加强了特征传播
* 加强了特征的重利用
* 减少了参数的数量



### 思路的创新

深度网络特深的后果是输入的信息或者梯度随着层次的加深而逐渐消失。

对于这个问题：

**ResNet，Highway Network，FractalNet 的共同特点就是在之前的层到之后的层之间创造较短的路径。**（尽管使用的是不同的方法）

**DenseNet**更加给力，把他们全部连起来。

这个思路比较与众不同，为啥呢？

因为其他的都是通过相加来合并特征，而DenseNet通过连接来合并特征。



### 为啥参数少

不需要重新学习冗余的特征图，啥意思呢？

来具体说说，DenseNet中区分对待加入网络的信息和保留信息。

大多数特征图保持不变，只有少量的特征图加入网络，最后分类器根据所有特征图做分类。

不想ResNet，每层都有自己的权重，但实际上好多层都没啥用，甚至可以在训练时随机抛弃。这些没啥用的layer的冗余参数使ResNet参数太多。



### DenseNet数学

提一下ResNet：

单层间的传播规则：

$y_l = h(x_l) + F(x_l,W_l)$

 $x_{l+1} = f(y_l)$

多层间传播的式子：

$x_L = x_l + \sum_{i=l}^{L}F(x_i,W_i) $ 

[ 其中，$x_L$代表了第L层的输入，$x_l$代表了第l层的输入，$F$代表了学习到的残差，也就是说ResNet的本质是学习层间的残差而完整地保留下来先前层的特征与数据信息 ]

提一下重要的DenseNet：

$$x_l = H_l([X_0,x_1,...,x_{l-1}])$$



### DenseNet具体结构

深度DenseNet大致结构：

1）三个Dense块，里面是DenseNet结构

2）块与块之间，连接层，用卷积和池化来抽取特征和调整特征图大小

具体的精妙提升性能细节：

1）narrow layer（生成新特征图速度慢）：DenseNet 的 layer 特别狭窄。特征图在DenseNet中差不多代表一种全局状态。DenseNet 每层都能接触到共同的集体知识，所以每层少生成一点特征图意味着每层对全局的影响小，一旦生成特征图，全局都能看到，没必要生成好多效果就很好。

2）瓶颈层：用1 * 1的卷积（瓶颈层）放在3 * 3的卷积前面，减少输入的特征图数量，特有效

3）压缩：在连接层里面去掉一点特征图，这样的网络叫Dense-C

DenseNet-BC ，又瓶颈层又压缩









​			

​			





